{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import os.path as osp\n",
    "import sys\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import *\n",
    "from utils.augmentations import SSDAugmentation\n",
    "from layers.modules import MultiBoxLoss\n",
    "from ssd import build_ssd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torchvision as tv\n",
    "import torch.optim as optim\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "import torchvision.datasets\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import argparse\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import cv2 \n",
    "import pickle as pkl\n",
    "import random\n",
    "import tarfile\n",
    "import collections\n",
    "import math\n",
    "import datetime\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from config_vgg16_ssd import *\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def xavier(param):\n",
    "    nn.init.xavier_uniform(param)\n",
    "\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        xavier(m.weight.data)\n",
    "        m.bias.data.zero_()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(device, resume=None):\n",
    "    \n",
    "    #use voc config \n",
    "    cfg = voc\n",
    " \n",
    "    ssd_net = build_ssd('train', cfg['min_dim'], cfg['num_classes'])\n",
    "    dataset = VOCDetection(root=dataset_root, image_sets=[('2012', 'train')],\n",
    "                            transform=SSDAugmentation(cfg['min_dim'],\n",
    "                             MEANS))\n",
    " \n",
    "\n",
    "    if resume:\n",
    "        print('Resuming training, loading previous training at ',resume)\n",
    "        ssd_net.load_weights(resume) \n",
    "    else:\n",
    "        vgg_weights = torch.load(basenet)\n",
    "        print('Loading base network...')\n",
    "        ssd_net.vgg.load_state_dict(vgg_weights)\n",
    "        print('Initializing weights...')\n",
    "        ssd_net.extras.apply(weights_init)\n",
    "        ssd_net.loc.apply(weights_init)\n",
    "        ssd_net.conf.apply(weights_init)\n",
    "    \n",
    "    net = ssd_net\n",
    "    \n",
    "    if device:\n",
    "        net = torch.nn.DataParallel(ssd_net)\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        net = net.to(device)\n",
    "        \n",
    "    optimizer = optim.SGD(net.parameters(), lr, momentum, weight_decay)\n",
    "    \n",
    "    criterion = MultiBoxLoss(cfg['num_classes'], 0.5, True, 0, True, 3, 0.5,\n",
    "                             False, torch.cuda.is_available())\n",
    "\n",
    "    \n",
    "    net.train()\n",
    "    mode = 'train'\n",
    "    \n",
    "\n",
    "    loc_loss = 0\n",
    "    conf_loss = 0\n",
    "    epoch = 0\n",
    "    print('Loading the dataset...')\n",
    "\n",
    "    epoch_size = len(dataset) // batch_size\n",
    "    print('Training SSD on: ',mode)\n",
    "\n",
    "    step_index = 0\n",
    "\n",
    "    #based on adapted code\n",
    "    train_data_loader = data.DataLoader(dataset, batch_size=32, num_workers=4, shuffle=True,\\\n",
    "                               collate_fn=detection_collate,pin_memory=True)\n",
    "    \n",
    "    print(\"Images in the training set = \" + str(len(dataset)))\n",
    "    print(\"Images in a mini-batch = \"+str(batch_size))\n",
    "    print(\"mini-batches = \" + str(len(train_data_loader)))\n",
    "    \n",
    "    \n",
    "     # create batch iterator\n",
    "    batch_iterator = iter(train_data_loader)\n",
    "    print(\"STARTING - ITERATIONS\")\n",
    "    \n",
    "    \n",
    "    l_loss = []\n",
    "    c_loss = []\n",
    "    itr = []\n",
    "    \n",
    "    #for 10000 iterations - takes long\n",
    "    for iteration in range(0, 10000):\n",
    "        \n",
    "        if iteration != 0 and (iteration % epoch_size == 0):\n",
    "            # reset epoch loss counters\n",
    "            loc_loss = 0\n",
    "            conf_loss = 0\n",
    "            epoch += 1\n",
    "\n",
    "        if iteration in cfg['lr_steps']:\n",
    "            step_index += 1\n",
    "            lr_dec = lr * (gamma ** (step_index))\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr_dec\n",
    "\n",
    "            \n",
    "        ## load train data\n",
    "        try:\n",
    "            images, targets = next(batch_iterator)\n",
    "        except StopIteration:\n",
    "            batch_iterator = iter(train_data_loader)\n",
    "            images, targets = next(batch_iterator)\n",
    "\n",
    "\n",
    "        \n",
    "        if device:\n",
    "            images = images.cuda()\n",
    "            targets = [ann.cuda() for ann in targets]\n",
    "        else:\n",
    "            images = images\n",
    "            targets = [ann for ann in targets]\n",
    "        \n",
    "        # forward\n",
    "        t0 = time.time()\n",
    "        out = net(images)\n",
    "\n",
    "        # backprop\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss_l, loss_c = criterion(out, targets)\n",
    "        loss = loss_l + loss_c\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        l_loss.append(loss_l.data.item())\n",
    "        c_loss.append(loss_c.data.item())\n",
    "        \n",
    "        t1 = time.time()\n",
    "        loc_loss += loss_l.data.item()\n",
    "        conf_loss += loss_c.data.item()\n",
    "        \n",
    "        itr.append(iteration)\n",
    "        \n",
    "        if iteration % 10 == 0:\n",
    "            print('timer: %.4f sec.' % (t1 - t0))\n",
    "            print('iter ' + repr(iteration) + ' || Loss: %.4f ||' % (loss.data.item()), end=' ')\n",
    "            currentDT = datetime.datetime.now()\n",
    "            print (currentDT.strftime(\"%H:%M:%S %p\"))\n",
    "            print(\"\\n\")\n",
    "        \n",
    "        \n",
    "        if iteration != 0 and iteration % 10 == 0:\n",
    "            print('Saving state, iter:', iteration)\n",
    "            iter_name = math.ceil(iteration/100)*100\n",
    "            torch.save(ssd_net.state_dict(), 'trained_weights/ssd_VOC_' +repr(iter_name) + '.pth')\n",
    "            with open('trained_weights/vgg16_ssd_stats_SGD.pkl','wb') as f:\n",
    "                pkl.dump([l_loss, c_loss, itr], f)\n",
    "                \n",
    "\n",
    "    torch.save(ssd_net.state_dict(),\n",
    "               save_folder + data_set + '.pth') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Initialize pointers\r\n",
      "data_set = 'VOC'\r\n",
      "dataset_root = voc_root = '//datasets/ee285f-public/PascalVOC2012/'\r\n",
      "save_folder = 'trained_weights/'\r\n",
      "eval_save_folder = 'eval/'\r\n",
      "devkit_path = 'devkit_path/'\r\n",
      "output_dir = \"out/\"\r\n",
      "\r\n",
      "#Run related metaparameters\r\n",
      "\r\n",
      "batch_size = 32\r\n",
      "resume = None\r\n",
      "\r\n",
      "#Optimization metaparameters\r\n",
      "lr = 1e-3\r\n",
      "momentum = 0.9\r\n",
      "weight_decay = 5e-4\r\n",
      "gamma = 0.1\r\n",
      "    \r\n",
      "confidence_threshold = 0.01\r\n",
      "top_k = 5\r\n",
      "cleanup = True\r\n",
      "\r\n",
      "YEAR = '2012'\r\n",
      "dataset_mean = (104, 117, 123)\r\n",
      "set_type = 'train'\r\n",
      "\r\n",
      "# Please Change if required\r\n",
      "trained_model = 'weights/ssd_pretrained.pth'\r\n",
      "basenet = 'weights/vgg16_reducedfc.pth'"
     ]
    }
   ],
   "source": [
    "!cat config_vgg16_ssd.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "change log in ssd.py\n",
      "Loading base network...\n",
      "Initializing weights...\n",
      "Loading the dataset...\n",
      "Training SSD on:  train\n",
      "Images in the training set = 5717\n",
      "Images in a mini-batch = 32\n",
      "mini-batches = 179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:2: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING - ITERATIONS\n",
      "timer: 1.6851 sec.\n",
      "iter 0 || Loss: 25.6439 || 14:14:46 PM\n",
      "\n",
      "\n",
      "timer: 0.8991 sec.\n",
      "iter 10 || Loss: 15.7061 || 14:15:06 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 10\n",
      "timer: 1.9832 sec.\n",
      "iter 20 || Loss: 15.2385 || 14:15:32 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 20\n",
      "timer: 0.8950 sec.\n",
      "iter 30 || Loss: 12.6622 || 14:15:52 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 30\n",
      "timer: 0.7953 sec.\n",
      "iter 40 || Loss: 11.5918 || 14:16:16 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 40\n",
      "timer: 1.0928 sec.\n",
      "iter 50 || Loss: 10.6089 || 14:16:39 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 50\n",
      "timer: 1.2910 sec.\n",
      "iter 60 || Loss: 9.9852 || 14:17:03 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 60\n",
      "timer: 0.8938 sec.\n",
      "iter 70 || Loss: 10.9167 || 14:17:26 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 70\n",
      "timer: 0.9930 sec.\n",
      "iter 80 || Loss: 10.5295 || 14:17:49 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 80\n",
      "timer: 1.1914 sec.\n",
      "iter 90 || Loss: 8.6384 || 14:18:14 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 90\n",
      "timer: 1.0954 sec.\n",
      "iter 100 || Loss: 7.9599 || 14:18:36 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 100\n",
      "timer: 0.8946 sec.\n",
      "iter 110 || Loss: 7.6604 || 14:19:01 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 110\n",
      "timer: 0.9954 sec.\n",
      "iter 120 || Loss: 7.4698 || 14:19:22 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 120\n",
      "timer: 1.0955 sec.\n",
      "iter 130 || Loss: 7.8378 || 14:19:49 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 130\n",
      "timer: 1.4054 sec.\n",
      "iter 140 || Loss: 7.5768 || 14:20:10 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 140\n",
      "timer: 0.7952 sec.\n",
      "iter 150 || Loss: 7.9485 || 14:20:36 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 150\n",
      "timer: 0.8885 sec.\n",
      "iter 160 || Loss: 7.4949 || 14:20:55 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 160\n",
      "timer: 1.3962 sec.\n",
      "iter 170 || Loss: 7.7227 || 14:21:21 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 170\n",
      "timer: 1.0869 sec.\n",
      "iter 180 || Loss: 8.1219 || 14:21:46 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 180\n",
      "timer: 1.2103 sec.\n",
      "iter 190 || Loss: 7.7252 || 14:22:07 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 190\n",
      "timer: 1.0961 sec.\n",
      "iter 200 || Loss: 8.5692 || 14:22:33 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 200\n",
      "timer: 1.3939 sec.\n",
      "iter 210 || Loss: 8.1043 || 14:22:53 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 210\n",
      "timer: 0.9928 sec.\n",
      "iter 220 || Loss: 7.3508 || 14:23:18 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 220\n",
      "timer: 1.1942 sec.\n",
      "iter 230 || Loss: 7.6788 || 14:23:39 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 230\n",
      "timer: 1.0999 sec.\n",
      "iter 240 || Loss: 6.7022 || 14:24:05 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 240\n",
      "timer: 1.1833 sec.\n",
      "iter 250 || Loss: 7.3127 || 14:24:26 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 250\n",
      "timer: 1.1139 sec.\n",
      "iter 260 || Loss: 7.0398 || 14:24:52 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 260\n",
      "timer: 0.7968 sec.\n",
      "iter 270 || Loss: 7.3768 || 14:25:12 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 270\n",
      "timer: 1.0991 sec.\n",
      "iter 280 || Loss: 6.6782 || 14:25:37 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 280\n",
      "timer: 0.8991 sec.\n",
      "iter 290 || Loss: 6.9648 || 14:25:58 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 290\n",
      "timer: 1.0953 sec.\n",
      "iter 300 || Loss: 6.9313 || 14:26:25 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 300\n",
      "timer: 1.1077 sec.\n",
      "iter 310 || Loss: 7.0357 || 14:26:46 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 310\n",
      "timer: 0.8940 sec.\n",
      "iter 320 || Loss: 6.6395 || 14:27:10 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 320\n",
      "timer: 1.0844 sec.\n",
      "iter 330 || Loss: 6.8419 || 14:27:33 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 330\n",
      "timer: 1.2947 sec.\n",
      "iter 340 || Loss: 7.2506 || 14:27:55 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 340\n",
      "timer: 1.0953 sec.\n",
      "iter 350 || Loss: 6.4424 || 14:28:19 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 350\n",
      "timer: 1.0952 sec.\n",
      "iter 360 || Loss: 7.2370 || 14:28:43 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 360\n",
      "timer: 1.1835 sec.\n",
      "iter 370 || Loss: 6.7818 || 14:29:06 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 370\n",
      "timer: 0.8944 sec.\n",
      "iter 380 || Loss: 6.9882 || 14:29:27 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 380\n",
      "timer: 0.8876 sec.\n",
      "iter 390 || Loss: 6.9811 || 14:29:53 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 390\n",
      "timer: 1.1723 sec.\n",
      "iter 400 || Loss: 6.6070 || 14:30:13 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 400\n",
      "timer: 1.3970 sec.\n",
      "iter 410 || Loss: 6.7339 || 14:30:39 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 410\n",
      "timer: 0.9990 sec.\n",
      "iter 420 || Loss: 6.7807 || 14:31:01 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 420\n",
      "timer: 1.0907 sec.\n",
      "iter 430 || Loss: 6.7788 || 14:31:26 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 430\n",
      "timer: 1.0912 sec.\n",
      "iter 440 || Loss: 7.3790 || 14:31:47 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 440\n",
      "timer: 1.1948 sec.\n",
      "iter 450 || Loss: 6.7218 || 14:32:09 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 450\n",
      "timer: 0.9793 sec.\n",
      "iter 460 || Loss: 6.7569 || 14:32:31 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 460\n",
      "timer: 0.8871 sec.\n",
      "iter 470 || Loss: 6.5827 || 14:32:55 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 470\n",
      "timer: 1.2954 sec.\n",
      "iter 480 || Loss: 6.7335 || 14:33:21 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 480\n",
      "timer: 0.8946 sec.\n",
      "iter 490 || Loss: 6.3251 || 14:33:41 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 490\n",
      "timer: 0.9093 sec.\n",
      "iter 500 || Loss: 6.1629 || 14:34:05 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 500\n",
      "timer: 1.5916 sec.\n",
      "iter 510 || Loss: 6.7429 || 14:34:27 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 510\n",
      "timer: 1.1990 sec.\n",
      "iter 520 || Loss: 6.3409 || 14:34:54 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 520\n",
      "timer: 1.3951 sec.\n",
      "iter 530 || Loss: 6.5138 || 14:35:14 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 530\n",
      "timer: 1.1963 sec.\n",
      "iter 540 || Loss: 6.4390 || 14:35:37 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 540\n",
      "timer: 1.3940 sec.\n",
      "iter 550 || Loss: 6.6887 || 14:36:03 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 550\n",
      "timer: 0.9959 sec.\n",
      "iter 560 || Loss: 6.1371 || 14:36:23 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 560\n",
      "timer: 0.6959 sec.\n",
      "iter 570 || Loss: 6.1194 || 14:36:49 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 570\n",
      "timer: 0.9019 sec.\n",
      "iter 580 || Loss: 6.5323 || 14:37:08 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 580\n",
      "timer: 1.2910 sec.\n",
      "iter 590 || Loss: 6.0078 || 14:37:33 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 590\n",
      "timer: 1.0963 sec.\n",
      "iter 600 || Loss: 6.3981 || 14:37:54 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 600\n",
      "timer: 0.7952 sec.\n",
      "iter 610 || Loss: 6.3059 || 14:38:20 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 610\n",
      "timer: 0.7946 sec.\n",
      "iter 620 || Loss: 6.5458 || 14:38:41 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 620\n",
      "timer: 1.0960 sec.\n",
      "iter 630 || Loss: 6.4751 || 14:39:07 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 630\n",
      "timer: 1.1947 sec.\n",
      "iter 640 || Loss: 6.5996 || 14:39:27 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 640\n",
      "timer: 1.0955 sec.\n",
      "iter 650 || Loss: 6.3763 || 14:39:52 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 650\n",
      "timer: 0.8948 sec.\n",
      "iter 660 || Loss: 6.5150 || 14:40:12 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 660\n",
      "timer: 1.0886 sec.\n",
      "iter 670 || Loss: 6.5468 || 14:40:38 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 670\n",
      "timer: 1.5944 sec.\n",
      "iter 680 || Loss: 6.2779 || 14:41:00 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 680\n",
      "timer: 0.9956 sec.\n",
      "iter 690 || Loss: 6.2199 || 14:41:24 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 690\n",
      "timer: 0.9970 sec.\n",
      "iter 700 || Loss: 6.0383 || 14:41:45 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 700\n",
      "timer: 0.7957 sec.\n",
      "iter 710 || Loss: 5.8057 || 14:42:09 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 710\n",
      "timer: 1.1952 sec.\n",
      "iter 720 || Loss: 6.0949 || 14:42:34 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 720\n",
      "timer: 0.9945 sec.\n",
      "iter 730 || Loss: 5.6823 || 14:42:55 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 730\n",
      "timer: 0.9834 sec.\n",
      "iter 740 || Loss: 5.9341 || 14:43:20 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 740\n",
      "timer: 0.9890 sec.\n",
      "iter 750 || Loss: 6.0696 || 14:43:40 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 750\n",
      "timer: 1.2919 sec.\n",
      "iter 760 || Loss: 6.0199 || 14:44:06 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 760\n",
      "timer: 1.2941 sec.\n",
      "iter 770 || Loss: 5.7112 || 14:44:27 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 770\n",
      "timer: 1.1983 sec.\n",
      "iter 780 || Loss: 6.4398 || 14:44:52 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 780\n",
      "timer: 0.8865 sec.\n",
      "iter 790 || Loss: 5.8825 || 14:45:16 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 790\n",
      "timer: 1.3759 sec.\n",
      "iter 800 || Loss: 6.1288 || 14:45:40 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 800\n",
      "timer: 1.0948 sec.\n",
      "iter 810 || Loss: 6.0378 || 14:46:01 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 810\n",
      "timer: 1.0919 sec.\n",
      "iter 820 || Loss: 6.2861 || 14:46:27 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 820\n",
      "timer: 1.3959 sec.\n",
      "iter 830 || Loss: 5.9836 || 14:46:49 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 830\n",
      "timer: 0.7950 sec.\n",
      "iter 840 || Loss: 6.0135 || 14:47:16 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 840\n",
      "timer: 0.8947 sec.\n",
      "iter 850 || Loss: 6.0550 || 14:47:34 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 850\n",
      "timer: 1.5810 sec.\n",
      "iter 860 || Loss: 6.0852 || 14:48:00 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 860\n",
      "timer: 0.8953 sec.\n",
      "iter 870 || Loss: 5.7515 || 14:48:21 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 870\n",
      "timer: 1.1934 sec.\n",
      "iter 880 || Loss: 5.8392 || 14:48:49 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 880\n",
      "timer: 0.6953 sec.\n",
      "iter 890 || Loss: 6.0388 || 14:49:06 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 890\n",
      "timer: 1.1944 sec.\n",
      "iter 900 || Loss: 5.3765 || 14:49:34 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 900\n",
      "timer: 0.8958 sec.\n",
      "iter 910 || Loss: 5.5905 || 14:49:56 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 910\n",
      "timer: 0.7942 sec.\n",
      "iter 920 || Loss: 5.8267 || 14:50:21 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 920\n",
      "timer: 0.8922 sec.\n",
      "iter 930 || Loss: 6.0279 || 14:50:43 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 930\n",
      "timer: 0.9948 sec.\n",
      "iter 940 || Loss: 5.4417 || 14:51:11 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 940\n",
      "timer: 1.1888 sec.\n",
      "iter 950 || Loss: 6.4686 || 14:51:29 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 950\n",
      "timer: 1.1721 sec.\n",
      "iter 960 || Loss: 5.9073 || 14:51:54 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 960\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timer: 0.9944 sec.\n",
      "iter 970 || Loss: 5.9176 || 14:52:12 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 970\n",
      "timer: 1.4011 sec.\n",
      "iter 980 || Loss: 5.7817 || 14:52:36 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 980\n",
      "timer: 0.9907 sec.\n",
      "iter 990 || Loss: 5.8546 || 14:52:58 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 990\n",
      "timer: 0.8964 sec.\n",
      "iter 1000 || Loss: 5.9577 || 14:53:22 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1000\n",
      "timer: 1.1307 sec.\n",
      "iter 1010 || Loss: 5.8597 || 14:53:45 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1010\n",
      "timer: 1.1876 sec.\n",
      "iter 1020 || Loss: 5.4567 || 14:54:08 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1020\n",
      "timer: 0.8940 sec.\n",
      "iter 1030 || Loss: 5.7475 || 14:54:30 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1030\n",
      "timer: 1.0954 sec.\n",
      "iter 1040 || Loss: 5.7011 || 14:54:55 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1040\n",
      "timer: 0.9949 sec.\n",
      "iter 1050 || Loss: 5.7843 || 14:55:17 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1050\n",
      "timer: 1.0783 sec.\n",
      "iter 1060 || Loss: 5.3935 || 14:55:42 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1060\n",
      "timer: 0.7963 sec.\n",
      "iter 1070 || Loss: 5.5035 || 14:56:02 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1070\n",
      "timer: 1.0958 sec.\n",
      "iter 1080 || Loss: 5.6670 || 14:56:27 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1080\n",
      "timer: 1.0995 sec.\n",
      "iter 1090 || Loss: 6.0395 || 14:56:53 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1090\n",
      "timer: 1.0949 sec.\n",
      "iter 1100 || Loss: 5.6293 || 14:57:13 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1100\n",
      "timer: 1.1790 sec.\n",
      "iter 1110 || Loss: 5.9531 || 14:57:39 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1110\n",
      "timer: 0.9118 sec.\n",
      "iter 1120 || Loss: 5.8429 || 14:58:00 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1120\n",
      "timer: 1.1735 sec.\n",
      "iter 1130 || Loss: 5.8116 || 14:58:26 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1130\n",
      "timer: 1.1917 sec.\n",
      "iter 1140 || Loss: 5.9695 || 14:58:46 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1140\n",
      "timer: 1.1705 sec.\n",
      "iter 1150 || Loss: 5.3901 || 14:59:10 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1150\n",
      "timer: 0.7081 sec.\n",
      "iter 1160 || Loss: 5.7698 || 14:59:30 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1160\n",
      "timer: 1.2859 sec.\n",
      "iter 1170 || Loss: 5.6108 || 14:59:57 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1170\n",
      "timer: 0.8941 sec.\n",
      "iter 1180 || Loss: 5.5776 || 15:00:16 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1180\n",
      "timer: 0.5760 sec.\n",
      "iter 1190 || Loss: 5.9504 || 15:00:40 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1190\n",
      "timer: 0.9884 sec.\n",
      "iter 1200 || Loss: 5.5833 || 15:01:02 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1200\n",
      "timer: 1.5718 sec.\n",
      "iter 1210 || Loss: 5.7754 || 15:01:28 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1210\n",
      "timer: 1.0087 sec.\n",
      "iter 1220 || Loss: 5.7858 || 15:01:47 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1220\n",
      "timer: 0.8037 sec.\n",
      "iter 1230 || Loss: 5.4251 || 15:02:12 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1230\n",
      "timer: 1.2800 sec.\n",
      "iter 1240 || Loss: 5.7081 || 15:02:34 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1240\n",
      "timer: 0.5986 sec.\n",
      "iter 1250 || Loss: 5.6166 || 15:02:56 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1250\n",
      "timer: 0.8947 sec.\n",
      "iter 1260 || Loss: 5.7901 || 15:03:22 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1260\n",
      "timer: 1.0952 sec.\n",
      "iter 1270 || Loss: 5.3045 || 15:03:52 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1270\n",
      "timer: 0.8949 sec.\n",
      "iter 1280 || Loss: 5.1027 || 15:04:12 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1280\n",
      "timer: 0.9823 sec.\n",
      "iter 1290 || Loss: 5.0738 || 15:04:37 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1290\n",
      "timer: 1.0910 sec.\n",
      "iter 1300 || Loss: 5.5779 || 15:04:57 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1300\n",
      "timer: 0.9949 sec.\n",
      "iter 1310 || Loss: 5.5443 || 15:05:23 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1310\n",
      "timer: 0.7944 sec.\n",
      "iter 1320 || Loss: 5.3925 || 15:05:42 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1320\n",
      "timer: 1.0962 sec.\n",
      "iter 1330 || Loss: 5.3476 || 15:06:10 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1330\n",
      "timer: 1.0947 sec.\n",
      "iter 1340 || Loss: 5.4930 || 15:06:29 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1340\n",
      "timer: 1.1934 sec.\n",
      "iter 1350 || Loss: 5.6111 || 15:06:57 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1350\n",
      "timer: 0.9909 sec.\n",
      "iter 1360 || Loss: 5.9089 || 15:07:19 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1360\n",
      "timer: 1.0953 sec.\n",
      "iter 1370 || Loss: 5.5675 || 15:07:43 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1370\n",
      "timer: 0.7949 sec.\n",
      "iter 1380 || Loss: 5.3765 || 15:08:03 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1380\n",
      "timer: 0.9947 sec.\n",
      "iter 1390 || Loss: 5.6013 || 15:08:30 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1390\n",
      "timer: 1.1951 sec.\n",
      "iter 1400 || Loss: 5.2933 || 15:08:50 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1400\n",
      "timer: 1.0956 sec.\n",
      "iter 1410 || Loss: 5.1827 || 15:09:16 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1410\n",
      "timer: 1.1017 sec.\n",
      "iter 1420 || Loss: 4.9798 || 15:09:37 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1420\n",
      "timer: 0.4305 sec.\n",
      "iter 1430 || Loss: 5.2901 || 15:09:54 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1430\n",
      "timer: 0.7990 sec.\n",
      "iter 1440 || Loss: 4.9821 || 15:10:25 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1440\n",
      "timer: 0.9003 sec.\n",
      "iter 1450 || Loss: 5.7133 || 15:10:46 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1450\n",
      "timer: 1.2154 sec.\n",
      "iter 1460 || Loss: 5.7788 || 15:11:10 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1460\n",
      "timer: 1.0971 sec.\n",
      "iter 1470 || Loss: 5.5806 || 15:11:31 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1470\n",
      "timer: 1.0906 sec.\n",
      "iter 1480 || Loss: 4.7109 || 15:11:57 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1480\n",
      "timer: 0.8959 sec.\n",
      "iter 1490 || Loss: 5.3569 || 15:12:19 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1490\n",
      "timer: 0.9976 sec.\n",
      "iter 1500 || Loss: 5.4142 || 15:12:44 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1500\n",
      "timer: 1.0959 sec.\n",
      "iter 1510 || Loss: 4.8601 || 15:13:06 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1510\n",
      "timer: 1.4811 sec.\n",
      "iter 1520 || Loss: 5.0623 || 15:13:28 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1520\n",
      "timer: 1.1945 sec.\n",
      "iter 1530 || Loss: 5.2213 || 15:13:52 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1530\n",
      "timer: 1.0947 sec.\n",
      "iter 1540 || Loss: 5.1637 || 15:14:14 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1540\n",
      "timer: 1.1950 sec.\n",
      "iter 1550 || Loss: 5.5002 || 15:14:37 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1550\n",
      "timer: 0.7875 sec.\n",
      "iter 1560 || Loss: 5.6903 || 15:14:59 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1560\n",
      "timer: 0.8955 sec.\n",
      "iter 1570 || Loss: 4.8079 || 15:15:23 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1570\n",
      "timer: 1.2001 sec.\n",
      "iter 1580 || Loss: 5.3405 || 15:15:48 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1580\n",
      "timer: 1.0049 sec.\n",
      "iter 1590 || Loss: 5.5280 || 15:16:09 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1590\n",
      "timer: 0.9821 sec.\n",
      "iter 1600 || Loss: 5.2650 || 15:16:36 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1600\n",
      "timer: 0.3140 sec.\n",
      "iter 1610 || Loss: 5.3869 || 15:16:53 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1610\n",
      "timer: 0.9908 sec.\n",
      "iter 1620 || Loss: 5.1713 || 15:17:24 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1620\n",
      "timer: 1.1955 sec.\n",
      "iter 1630 || Loss: 5.3921 || 15:17:45 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1630\n",
      "timer: 1.1893 sec.\n",
      "iter 1640 || Loss: 4.9339 || 15:18:09 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1640\n",
      "timer: 1.0941 sec.\n",
      "iter 1650 || Loss: 4.6016 || 15:18:30 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1650\n",
      "timer: 1.4813 sec.\n",
      "iter 1660 || Loss: 5.0466 || 15:18:55 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1660\n",
      "timer: 0.9954 sec.\n",
      "iter 1670 || Loss: 5.4063 || 15:19:16 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1670\n",
      "timer: 1.0824 sec.\n",
      "iter 1680 || Loss: 5.1761 || 15:19:42 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1680\n",
      "timer: 1.0888 sec.\n",
      "iter 1690 || Loss: 5.2055 || 15:20:03 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1690\n",
      "timer: 0.8850 sec.\n",
      "iter 1700 || Loss: 5.3385 || 15:20:30 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1700\n",
      "timer: 1.0955 sec.\n",
      "iter 1710 || Loss: 5.1281 || 15:20:51 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1710\n",
      "timer: 0.9927 sec.\n",
      "iter 1720 || Loss: 5.5535 || 15:21:18 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1720\n",
      "timer: 0.9942 sec.\n",
      "iter 1730 || Loss: 4.8666 || 15:21:37 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1730\n",
      "timer: 0.9923 sec.\n",
      "iter 1740 || Loss: 5.2105 || 15:22:05 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1740\n",
      "timer: 1.2901 sec.\n",
      "iter 1750 || Loss: 4.9212 || 15:22:23 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1750\n",
      "timer: 0.9893 sec.\n",
      "iter 1760 || Loss: 5.4361 || 15:22:47 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1760\n",
      "timer: 1.2930 sec.\n",
      "iter 1770 || Loss: 4.9181 || 15:23:09 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1770\n",
      "timer: 0.8879 sec.\n",
      "iter 1780 || Loss: 5.1938 || 15:23:32 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1780\n",
      "timer: 1.1808 sec.\n",
      "iter 1790 || Loss: 4.9228 || 15:23:53 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1790\n",
      "timer: 1.2873 sec.\n",
      "iter 1800 || Loss: 5.3036 || 15:24:15 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1800\n",
      "timer: 0.7931 sec.\n",
      "iter 1810 || Loss: 5.2665 || 15:24:36 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1810\n",
      "timer: 0.9932 sec.\n",
      "iter 1820 || Loss: 4.9202 || 15:24:59 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1820\n",
      "timer: 0.8902 sec.\n",
      "iter 1830 || Loss: 5.3906 || 15:25:23 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1830\n",
      "timer: 1.1156 sec.\n",
      "iter 1840 || Loss: 5.1497 || 15:25:46 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1840\n",
      "timer: 1.0804 sec.\n",
      "iter 1850 || Loss: 5.3693 || 15:26:10 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1850\n",
      "timer: 0.9855 sec.\n",
      "iter 1860 || Loss: 4.9094 || 15:26:32 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1860\n",
      "timer: 1.1848 sec.\n",
      "iter 1870 || Loss: 5.0531 || 15:26:57 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1870\n",
      "timer: 0.8989 sec.\n",
      "iter 1880 || Loss: 5.1274 || 15:27:19 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1880\n",
      "timer: 0.7950 sec.\n",
      "iter 1890 || Loss: 5.1577 || 15:27:40 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1890\n",
      "timer: 0.9960 sec.\n",
      "iter 1900 || Loss: 5.4244 || 15:28:05 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1900\n",
      "timer: 1.0836 sec.\n",
      "iter 1910 || Loss: 4.8366 || 15:28:27 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1910\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timer: 1.1949 sec.\n",
      "iter 1920 || Loss: 4.9983 || 15:28:53 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1920\n",
      "timer: 1.0895 sec.\n",
      "iter 1930 || Loss: 4.6338 || 15:29:20 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1930\n",
      "timer: 1.0887 sec.\n",
      "iter 1940 || Loss: 5.2384 || 15:29:44 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1940\n",
      "timer: 1.2854 sec.\n",
      "iter 1950 || Loss: 4.8241 || 15:30:09 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1950\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-e0b89bcb91b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresume\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-19-59f2c9399964>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(device, resume)\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0mloss_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_l\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_c\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mlip_proj/final/ssd-master_qfgaohao/layers/modules/multibox_loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, predictions, targets)\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0mdefaults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpriors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             match(self.threshold, truths, defaults, self.variance, labels,\n\u001b[0;32m---> 73\u001b[0;31m                   loc_t, conf_t, idx)\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_gpu\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mloc_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloc_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mlip_proj/final/ssd-master_qfgaohao/layers/box_utils.py\u001b[0m in \u001b[0;36mmatch\u001b[0;34m(threshold, truths, priors, variances, labels, loc_t, conf_t, idx)\u001b[0m\n\u001b[1;32m     88\u001b[0m     overlaps = jaccard(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtruths\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0mpoint_form\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpriors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m     )\n\u001b[1;32m     92\u001b[0m     \u001b[0;31m# (Bipartite Matching)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mlip_proj/final/ssd-master_qfgaohao/layers/box_utils.py\u001b[0m in \u001b[0;36mpoint_form\u001b[0;34m(boxes)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \"\"\"\n\u001b[1;32m     12\u001b[0m     return torch.cat((boxes[:, :2] - boxes[:, 2:]/2,     # xmin, ymin\n\u001b[0;32m---> 13\u001b[0;31m                      boxes[:, :2] + boxes[:, 2:]/2), 1)  # xmax, ymax\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(device, resume)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Loss')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open('trained_weights/vgg16_ssd_stats_SGD.pkl','rb') as f:\n",
    "    l_loss, c_loss, itr = pickle.load(f)\n",
    "\n",
    "l_loss = np.asarray(l_loss)\n",
    "c_loss = np.asarray(c_loss)\n",
    "itr = np.asarray(itr)\n",
    "plt.plot(itr,l_loss+c_loss)\n",
    "plt.title('Loss - Iterations: lr=1e-3  SGD Optimizer')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
