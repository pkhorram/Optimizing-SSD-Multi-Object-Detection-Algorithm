{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import os.path as osp\n",
    "import sys\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import *\n",
    "from utils.augmentations import SSDAugmentation\n",
    "from layers.modules import MultiBoxLoss\n",
    "from ssd import build_ssd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torchvision as tv\n",
    "import torch.optim as optim\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "import torchvision.datasets\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import argparse\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import cv2 \n",
    "import pickle as pkl\n",
    "import random\n",
    "import tarfile\n",
    "import collections\n",
    "import math\n",
    "import datetime\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from config_vgg16_ssd import *\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def xavier(param):\n",
    "    nn.init.xavier_uniform(param)\n",
    "\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        xavier(m.weight.data)\n",
    "        m.bias.data.zero_()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(device, resume=None):\n",
    "    \n",
    "    #use voc config \n",
    "    cfg = voc\n",
    " \n",
    "    ssd_net = build_ssd('train', cfg['min_dim'], cfg['num_classes'])\n",
    "    dataset = VOCDetection(root=dataset_root, image_sets=[('2012', 'train')],\n",
    "                            transform=SSDAugmentation(cfg['min_dim'],\n",
    "                             MEANS))\n",
    " \n",
    "\n",
    "    if resume:\n",
    "        print('Resuming training, loading previous training at ',resume)\n",
    "        ssd_net.load_weights(resume) \n",
    "    else:\n",
    "        vgg_weights = torch.load(basenet)\n",
    "        print('Loading base network...')\n",
    "        ssd_net.vgg.load_state_dict(vgg_weights)\n",
    "        print('Initializing weights...')\n",
    "        ssd_net.extras.apply(weights_init)\n",
    "        ssd_net.loc.apply(weights_init)\n",
    "        ssd_net.conf.apply(weights_init)\n",
    "    \n",
    "    net = ssd_net\n",
    "    \n",
    "    if device:\n",
    "        net = torch.nn.DataParallel(ssd_net)\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        net = net.to(device)\n",
    " \n",
    "    optimizer = optim.RMSprop(net.parameters(), lr=lr,weight_decay=weight_decay)  \n",
    "    \n",
    "    criterion = MultiBoxLoss(cfg['num_classes'], 0.5, True, 0, True, 3, 0.5,\n",
    "                             False, torch.cuda.is_available())\n",
    "\n",
    "    \n",
    "    net.train()\n",
    "    mode = 'train'\n",
    "    \n",
    "\n",
    "    loc_loss = 0\n",
    "    conf_loss = 0\n",
    "    epoch = 0\n",
    "    print('Loading the dataset...')\n",
    "\n",
    "    epoch_size = len(dataset) // batch_size\n",
    "    print('Training SSD on: ',mode)\n",
    "\n",
    "    step_index = 0\n",
    "\n",
    "    #based on adapted code\n",
    "    train_data_loader = data.DataLoader(dataset, batch_size=32, num_workers=4, shuffle=True,\\\n",
    "                               collate_fn=detection_collate,pin_memory=True)\n",
    "    \n",
    "    print(\"Images in the training set = \" + str(len(dataset)))\n",
    "    print(\"Images in a mini-batch = \"+str(batch_size))\n",
    "    print(\"mini-batches = \" + str(len(train_data_loader)))\n",
    "    \n",
    "    \n",
    "     # create batch iterator\n",
    "    batch_iterator = iter(train_data_loader)\n",
    "    print(\"STARTING - ITERATIONS\")\n",
    "    \n",
    "    \n",
    "    l_loss = []\n",
    "    c_loss = []\n",
    "    itr = []\n",
    "    \n",
    "    #for 10000 iterations - takes long\n",
    "    for iteration in range(0, 2000):\n",
    "        \n",
    "        if iteration != 0 and (iteration % epoch_size == 0):\n",
    "            # reset epoch loss counters\n",
    "            loc_loss = 0\n",
    "            conf_loss = 0\n",
    "            epoch += 1\n",
    "\n",
    "        if iteration in cfg['lr_steps']:\n",
    "            step_index += 1\n",
    "            lr_dec = lr * (gamma ** (step_index))\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr_dec\n",
    "\n",
    "            \n",
    "        ## load train data\n",
    "        try:\n",
    "            images, targets = next(batch_iterator)\n",
    "        except StopIteration:\n",
    "            batch_iterator = iter(train_data_loader)\n",
    "            images, targets = next(batch_iterator)\n",
    "\n",
    "\n",
    "        \n",
    "        if device:\n",
    "            images = images.cuda()\n",
    "            targets = [ann.cuda() for ann in targets]\n",
    "        else:\n",
    "            images = images\n",
    "            targets = [ann for ann in targets]\n",
    "        \n",
    "        # forward\n",
    "        t0 = time.time()\n",
    "        out = net(images)\n",
    "\n",
    "        # backprop\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss_l, loss_c = criterion(out, targets)\n",
    "        loss = loss_l + loss_c\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        l_loss.append(loss_l.data.item())\n",
    "        c_loss.append(loss_c.data.item())\n",
    "        \n",
    "        t1 = time.time()\n",
    "        loc_loss += loss_l.data.item()\n",
    "        conf_loss += loss_c.data.item()\n",
    "        \n",
    "        itr.append(iteration)\n",
    "        \n",
    "        if iteration % 10 == 0:\n",
    "            print('timer: %.4f sec.' % (t1 - t0))\n",
    "            print('iter ' + repr(iteration) + ' || Loss: %.4f ||' % (loss.data.item()), end=' ')\n",
    "            currentDT = datetime.datetime.now()\n",
    "            print (currentDT.strftime(\"%H:%M:%S %p\"))\n",
    "            print(\"\\n\")\n",
    "        \n",
    "        \n",
    "        if iteration != 0 and iteration % 100 == 0:\n",
    "            print('Saving state, iter:', iteration)\n",
    "            iter_name = math.ceil(iteration/100)*100\n",
    "            torch.save(ssd_net.state_dict(), 'trained_weights/ssd_VOC_RMSprop_' +repr(iter_name) + '.pth')\n",
    "            with open('trained_weights/vgg16_ssd_stats_RMSprop.pkl','wb') as f:\n",
    "                pkl.dump([l_loss, c_loss, itr], f)\n",
    "                \n",
    "\n",
    "    torch.save(ssd_net.state_dict(),\n",
    "               save_folder + data_set + '.pth') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Initialize pointers\r\n",
      "data_set = 'VOC'\r\n",
      "dataset_root = voc_root = '//datasets/ee285f-public/PascalVOC2012/'\r\n",
      "save_folder = 'trained_weights/'\r\n",
      "eval_save_folder = 'eval/'\r\n",
      "devkit_path = 'devkit_path/'\r\n",
      "output_dir = \"out/\"\r\n",
      "\r\n",
      "#Run related metaparameters\r\n",
      "\r\n",
      "batch_size = 32\r\n",
      "resume = None\r\n",
      "\r\n",
      "#Optimization metaparameters\r\n",
      "lr = 1e-3\r\n",
      "momentum = 0.9\r\n",
      "weight_decay = 5e-4\r\n",
      "gamma = 0.1\r\n",
      "    \r\n",
      "confidence_threshold = 0.01\r\n",
      "top_k = 5\r\n",
      "cleanup = True\r\n",
      "\r\n",
      "YEAR = '2012'\r\n",
      "dataset_mean = (104, 117, 123)\r\n",
      "set_type = 'train'\r\n",
      "\r\n",
      "# Please Change if required\r\n",
      "trained_model = 'weights/ssd_pretrained.pth'\r\n",
      "basenet = 'weights/vgg16_reducedfc.pth'"
     ]
    }
   ],
   "source": [
    "!cat config_vgg16_ssd.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "change log in ssd.py\n",
      "Loading base network...\n",
      "Initializing weights...\n",
      "Loading the dataset...\n",
      "Training SSD on:  train\n",
      "Images in the training set = 5717\n",
      "Images in a mini-batch = 32\n",
      "mini-batches = 179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:2: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING - ITERATIONS\n",
      "timer: 0.5934 sec.\n",
      "iter 0 || Loss: 26.4075 || 23:08:20 PM\n",
      "\n",
      "\n",
      "timer: 0.6966 sec.\n",
      "iter 10 || Loss: 25.1958 || 23:08:27 PM\n",
      "\n",
      "\n",
      "timer: 0.5706 sec.\n",
      "iter 20 || Loss: 17.7655 || 23:08:34 PM\n",
      "\n",
      "\n",
      "timer: 0.6960 sec.\n",
      "iter 30 || Loss: 12.0922 || 23:08:42 PM\n",
      "\n",
      "\n",
      "timer: 0.7648 sec.\n",
      "iter 40 || Loss: 9.8675 || 23:08:49 PM\n",
      "\n",
      "\n",
      "timer: 0.5860 sec.\n",
      "iter 50 || Loss: 9.7384 || 23:08:56 PM\n",
      "\n",
      "\n",
      "timer: 0.6960 sec.\n",
      "iter 60 || Loss: 8.4308 || 23:09:05 PM\n",
      "\n",
      "\n",
      "timer: 0.6958 sec.\n",
      "iter 70 || Loss: 8.7038 || 23:09:12 PM\n",
      "\n",
      "\n",
      "timer: 0.6975 sec.\n",
      "iter 80 || Loss: 7.7706 || 23:09:19 PM\n",
      "\n",
      "\n",
      "timer: 0.6988 sec.\n",
      "iter 90 || Loss: 8.1916 || 23:09:26 PM\n",
      "\n",
      "\n",
      "timer: 0.6942 sec.\n",
      "iter 100 || Loss: 8.3539 || 23:09:33 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 100\n",
      "timer: 0.5846 sec.\n",
      "iter 110 || Loss: 8.1949 || 23:09:40 PM\n",
      "\n",
      "\n",
      "timer: 0.6680 sec.\n",
      "iter 120 || Loss: 7.9761 || 23:09:48 PM\n",
      "\n",
      "\n",
      "timer: 0.6723 sec.\n",
      "iter 130 || Loss: 8.0800 || 23:09:55 PM\n",
      "\n",
      "\n",
      "timer: 0.6955 sec.\n",
      "iter 140 || Loss: 7.8857 || 23:10:03 PM\n",
      "\n",
      "\n",
      "timer: 0.6725 sec.\n",
      "iter 150 || Loss: 7.4961 || 23:10:11 PM\n",
      "\n",
      "\n",
      "timer: 0.7965 sec.\n",
      "iter 160 || Loss: 8.0182 || 23:10:18 PM\n",
      "\n",
      "\n",
      "timer: 0.7079 sec.\n",
      "iter 170 || Loss: 8.6348 || 23:10:26 PM\n",
      "\n",
      "\n",
      "timer: 0.8003 sec.\n",
      "iter 180 || Loss: 8.1409 || 23:10:35 PM\n",
      "\n",
      "\n",
      "timer: 0.6934 sec.\n",
      "iter 190 || Loss: 7.4342 || 23:10:43 PM\n",
      "\n",
      "\n",
      "timer: 0.6681 sec.\n",
      "iter 200 || Loss: 8.3102 || 23:10:50 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 200\n",
      "timer: 0.5963 sec.\n",
      "iter 210 || Loss: 8.0764 || 23:10:57 PM\n",
      "\n",
      "\n",
      "timer: 0.5954 sec.\n",
      "iter 220 || Loss: 7.5618 || 23:11:04 PM\n",
      "\n",
      "\n",
      "timer: 0.5962 sec.\n",
      "iter 230 || Loss: 8.3024 || 23:11:11 PM\n",
      "\n",
      "\n",
      "timer: 0.6030 sec.\n",
      "iter 240 || Loss: 7.4377 || 23:11:18 PM\n",
      "\n",
      "\n",
      "timer: 0.5954 sec.\n",
      "iter 250 || Loss: 7.9564 || 23:11:26 PM\n",
      "\n",
      "\n",
      "timer: 0.6988 sec.\n",
      "iter 260 || Loss: 7.5891 || 23:11:33 PM\n",
      "\n",
      "\n",
      "timer: 0.6929 sec.\n",
      "iter 270 || Loss: 7.5432 || 23:11:40 PM\n",
      "\n",
      "\n",
      "timer: 0.5928 sec.\n",
      "iter 280 || Loss: 7.6985 || 23:11:48 PM\n",
      "\n",
      "\n",
      "timer: 0.5963 sec.\n",
      "iter 290 || Loss: 6.9733 || 23:11:55 PM\n",
      "\n",
      "\n",
      "timer: 0.6833 sec.\n",
      "iter 300 || Loss: 7.6837 || 23:12:03 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 300\n",
      "timer: 0.5136 sec.\n",
      "iter 310 || Loss: 7.8555 || 23:12:11 PM\n",
      "\n",
      "\n",
      "timer: 0.6373 sec.\n",
      "iter 320 || Loss: 7.6648 || 23:12:18 PM\n",
      "\n",
      "\n",
      "timer: 0.7959 sec.\n",
      "iter 330 || Loss: 7.8822 || 23:12:25 PM\n",
      "\n",
      "\n",
      "timer: 0.5997 sec.\n",
      "iter 340 || Loss: 8.0770 || 23:12:32 PM\n",
      "\n",
      "\n",
      "timer: 0.6961 sec.\n",
      "iter 350 || Loss: 7.4422 || 23:12:40 PM\n",
      "\n",
      "\n",
      "timer: 0.6965 sec.\n",
      "iter 360 || Loss: 7.1762 || 23:12:49 PM\n",
      "\n",
      "\n",
      "timer: 0.6751 sec.\n",
      "iter 370 || Loss: 8.5395 || 23:12:57 PM\n",
      "\n",
      "\n",
      "timer: 0.7958 sec.\n",
      "iter 380 || Loss: 7.8194 || 23:13:04 PM\n",
      "\n",
      "\n",
      "timer: 0.6801 sec.\n",
      "iter 390 || Loss: 7.6327 || 23:13:11 PM\n",
      "\n",
      "\n",
      "timer: 0.7961 sec.\n",
      "iter 400 || Loss: 7.3999 || 23:13:19 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 400\n",
      "timer: 0.7966 sec.\n",
      "iter 410 || Loss: 7.3794 || 23:13:26 PM\n",
      "\n",
      "\n",
      "timer: 0.6961 sec.\n",
      "iter 420 || Loss: 7.1780 || 23:13:33 PM\n",
      "\n",
      "\n",
      "timer: 0.6956 sec.\n",
      "iter 430 || Loss: 7.8007 || 23:13:40 PM\n",
      "\n",
      "\n",
      "timer: 0.5962 sec.\n",
      "iter 440 || Loss: 7.0597 || 23:13:48 PM\n",
      "\n",
      "\n",
      "timer: 0.6678 sec.\n",
      "iter 450 || Loss: 7.6489 || 23:13:55 PM\n",
      "\n",
      "\n",
      "timer: 0.5979 sec.\n",
      "iter 460 || Loss: 7.0981 || 23:14:02 PM\n",
      "\n",
      "\n",
      "timer: 0.6743 sec.\n",
      "iter 470 || Loss: 7.8813 || 23:14:10 PM\n",
      "\n",
      "\n",
      "timer: 0.5951 sec.\n",
      "iter 480 || Loss: 7.2384 || 23:14:17 PM\n",
      "\n",
      "\n",
      "timer: 0.7596 sec.\n",
      "iter 490 || Loss: 8.4244 || 23:14:24 PM\n",
      "\n",
      "\n",
      "timer: 0.5960 sec.\n",
      "iter 500 || Loss: 8.3451 || 23:14:31 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 500\n",
      "timer: 0.6960 sec.\n",
      "iter 510 || Loss: 9.0907 || 23:14:38 PM\n",
      "\n",
      "\n",
      "timer: 0.6955 sec.\n",
      "iter 520 || Loss: 7.3368 || 23:14:46 PM\n",
      "\n",
      "\n",
      "timer: 0.7964 sec.\n",
      "iter 530 || Loss: 9.0500 || 23:14:53 PM\n",
      "\n",
      "\n",
      "timer: 0.6967 sec.\n",
      "iter 540 || Loss: 10.4507 || 23:15:03 PM\n",
      "\n",
      "\n",
      "timer: 0.6965 sec.\n",
      "iter 550 || Loss: 9.1369 || 23:15:11 PM\n",
      "\n",
      "\n",
      "timer: 0.5966 sec.\n",
      "iter 560 || Loss: 8.8109 || 23:15:18 PM\n",
      "\n",
      "\n",
      "timer: 0.7960 sec.\n",
      "iter 570 || Loss: 7.6609 || 23:15:25 PM\n",
      "\n",
      "\n",
      "timer: 0.5958 sec.\n",
      "iter 580 || Loss: 7.8137 || 23:15:33 PM\n",
      "\n",
      "\n",
      "timer: 0.5964 sec.\n",
      "iter 590 || Loss: 7.7763 || 23:15:40 PM\n",
      "\n",
      "\n",
      "timer: 0.5819 sec.\n",
      "iter 600 || Loss: 7.3591 || 23:15:47 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 600\n",
      "timer: 0.6966 sec.\n",
      "iter 610 || Loss: 7.2101 || 23:15:55 PM\n",
      "\n",
      "\n",
      "timer: 0.5956 sec.\n",
      "iter 620 || Loss: 7.4953 || 23:16:06 PM\n",
      "\n",
      "\n",
      "timer: 0.5949 sec.\n",
      "iter 630 || Loss: 7.4817 || 23:16:13 PM\n",
      "\n",
      "\n",
      "timer: 0.5907 sec.\n",
      "iter 640 || Loss: 7.4493 || 23:16:20 PM\n",
      "\n",
      "\n",
      "timer: 0.5966 sec.\n",
      "iter 650 || Loss: 7.1650 || 23:16:27 PM\n",
      "\n",
      "\n",
      "timer: 0.5957 sec.\n",
      "iter 660 || Loss: 7.5917 || 23:16:34 PM\n",
      "\n",
      "\n",
      "timer: 0.6964 sec.\n",
      "iter 670 || Loss: 7.4991 || 23:16:42 PM\n",
      "\n",
      "\n",
      "timer: 0.6905 sec.\n",
      "iter 680 || Loss: 6.8212 || 23:16:50 PM\n",
      "\n",
      "\n",
      "timer: 0.5971 sec.\n",
      "iter 690 || Loss: 7.2902 || 23:16:57 PM\n",
      "\n",
      "\n",
      "timer: 0.6965 sec.\n",
      "iter 700 || Loss: 10.0071 || 23:17:04 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 700\n",
      "timer: 0.4965 sec.\n",
      "iter 710 || Loss: 9.3791 || 23:17:11 PM\n",
      "\n",
      "\n",
      "timer: 0.7923 sec.\n",
      "iter 720 || Loss: 12.3413 || 23:17:21 PM\n",
      "\n",
      "\n",
      "timer: 0.5959 sec.\n",
      "iter 730 || Loss: 15.0665 || 23:17:28 PM\n",
      "\n",
      "\n",
      "timer: 0.6886 sec.\n",
      "iter 740 || Loss: 13.8057 || 23:17:35 PM\n",
      "\n",
      "\n",
      "timer: 0.6030 sec.\n",
      "iter 750 || Loss: 10.2979 || 23:17:42 PM\n",
      "\n",
      "\n",
      "timer: 0.5894 sec.\n",
      "iter 760 || Loss: 9.7681 || 23:17:50 PM\n",
      "\n",
      "\n",
      "timer: 0.5990 sec.\n",
      "iter 770 || Loss: 9.0134 || 23:17:57 PM\n",
      "\n",
      "\n",
      "timer: 0.5712 sec.\n",
      "iter 780 || Loss: 7.9110 || 23:18:04 PM\n",
      "\n",
      "\n",
      "timer: 0.5967 sec.\n",
      "iter 790 || Loss: 8.0221 || 23:18:12 PM\n",
      "\n",
      "\n",
      "timer: 0.7807 sec.\n",
      "iter 800 || Loss: 7.6294 || 23:18:21 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 800\n",
      "timer: 0.5956 sec.\n",
      "iter 810 || Loss: 11.3964 || 23:18:28 PM\n",
      "\n",
      "\n",
      "timer: 0.6922 sec.\n",
      "iter 820 || Loss: 8.0760 || 23:18:36 PM\n",
      "\n",
      "\n",
      "timer: 0.7958 sec.\n",
      "iter 830 || Loss: 8.4175 || 23:18:44 PM\n",
      "\n",
      "\n",
      "timer: 0.5837 sec.\n",
      "iter 840 || Loss: 7.6488 || 23:18:51 PM\n",
      "\n",
      "\n",
      "timer: 0.5960 sec.\n",
      "iter 850 || Loss: 8.2463 || 23:18:59 PM\n",
      "\n",
      "\n",
      "timer: 0.8714 sec.\n",
      "iter 860 || Loss: 7.7552 || 23:19:07 PM\n",
      "\n",
      "\n",
      "timer: 0.5961 sec.\n",
      "iter 870 || Loss: 8.1270 || 23:19:15 PM\n",
      "\n",
      "\n",
      "timer: 0.6814 sec.\n",
      "iter 880 || Loss: 8.2037 || 23:19:23 PM\n",
      "\n",
      "\n",
      "timer: 0.5963 sec.\n",
      "iter 890 || Loss: 7.6187 || 23:19:30 PM\n",
      "\n",
      "\n",
      "timer: 0.6963 sec.\n",
      "iter 900 || Loss: 8.4109 || 23:19:39 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 900\n",
      "timer: 0.6131 sec.\n",
      "iter 910 || Loss: 7.7352 || 23:19:46 PM\n",
      "\n",
      "\n",
      "timer: 0.5963 sec.\n",
      "iter 920 || Loss: 9.0923 || 23:19:54 PM\n",
      "\n",
      "\n",
      "timer: 0.6962 sec.\n",
      "iter 930 || Loss: 8.6742 || 23:20:02 PM\n",
      "\n",
      "\n",
      "timer: 0.6950 sec.\n",
      "iter 940 || Loss: 28.0899 || 23:20:09 PM\n",
      "\n",
      "\n",
      "timer: 0.6870 sec.\n",
      "iter 950 || Loss: 12.0809 || 23:20:16 PM\n",
      "\n",
      "\n",
      "timer: 0.5955 sec.\n",
      "iter 960 || Loss: 8.6319 || 23:20:24 PM\n",
      "\n",
      "\n",
      "timer: 0.6062 sec.\n",
      "iter 970 || Loss: 8.0773 || 23:20:31 PM\n",
      "\n",
      "\n",
      "timer: 0.7774 sec.\n",
      "iter 980 || Loss: 11.5847 || 23:20:39 PM\n",
      "\n",
      "\n",
      "timer: 0.5965 sec.\n",
      "iter 990 || Loss: 8.6580 || 23:20:46 PM\n",
      "\n",
      "\n",
      "timer: 0.5952 sec.\n",
      "iter 1000 || Loss: 8.4934 || 23:20:54 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1000\n",
      "timer: 0.5957 sec.\n",
      "iter 1010 || Loss: 9.7833 || 23:21:01 PM\n",
      "\n",
      "\n",
      "timer: 0.7962 sec.\n",
      "iter 1020 || Loss: 8.2611 || 23:21:09 PM\n",
      "\n",
      "\n",
      "timer: 0.6958 sec.\n",
      "iter 1030 || Loss: 9.1105 || 23:21:16 PM\n",
      "\n",
      "\n",
      "timer: 0.5965 sec.\n",
      "iter 1040 || Loss: 7.8606 || 23:21:24 PM\n",
      "\n",
      "\n",
      "timer: 0.6959 sec.\n",
      "iter 1050 || Loss: 8.1738 || 23:21:31 PM\n",
      "\n",
      "\n",
      "timer: 0.6964 sec.\n",
      "iter 1060 || Loss: 8.3712 || 23:21:38 PM\n",
      "\n",
      "\n",
      "timer: 0.4691 sec.\n",
      "iter 1070 || Loss: 7.8176 || 23:21:45 PM\n",
      "\n",
      "\n",
      "timer: 0.5957 sec.\n",
      "iter 1080 || Loss: 7.6645 || 23:21:55 PM\n",
      "\n",
      "\n",
      "timer: 0.7730 sec.\n",
      "iter 1090 || Loss: 8.0731 || 23:22:02 PM\n",
      "\n",
      "\n",
      "timer: 0.5962 sec.\n",
      "iter 1100 || Loss: 7.7131 || 23:22:09 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1100\n",
      "timer: 0.7790 sec.\n",
      "iter 1110 || Loss: 9.0724 || 23:22:16 PM\n",
      "\n",
      "\n",
      "timer: 0.6966 sec.\n",
      "iter 1120 || Loss: 49.9301 || 23:22:24 PM\n",
      "\n",
      "\n",
      "timer: 0.6841 sec.\n",
      "iter 1130 || Loss: 10.1920 || 23:22:31 PM\n",
      "\n",
      "\n",
      "timer: 0.6624 sec.\n",
      "iter 1140 || Loss: 8.8520 || 23:22:39 PM\n",
      "\n",
      "\n",
      "timer: 0.6948 sec.\n",
      "iter 1150 || Loss: 8.8864 || 23:22:47 PM\n",
      "\n",
      "\n",
      "timer: 0.5947 sec.\n",
      "iter 1160 || Loss: 9.4572 || 23:22:54 PM\n",
      "\n",
      "\n",
      "timer: 0.6810 sec.\n",
      "iter 1170 || Loss: 7.6486 || 23:23:01 PM\n",
      "\n",
      "\n",
      "timer: 0.5961 sec.\n",
      "iter 1180 || Loss: 7.6280 || 23:23:08 PM\n",
      "\n",
      "\n",
      "timer: 0.7709 sec.\n",
      "iter 1190 || Loss: 8.2867 || 23:23:17 PM\n",
      "\n",
      "\n",
      "timer: 0.5967 sec.\n",
      "iter 1200 || Loss: 7.7729 || 23:23:24 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1200\n",
      "timer: 0.6712 sec.\n",
      "iter 1210 || Loss: 8.6181 || 23:23:32 PM\n",
      "\n",
      "\n",
      "timer: 0.5961 sec.\n",
      "iter 1220 || Loss: 8.1255 || 23:23:39 PM\n",
      "\n",
      "\n",
      "timer: 0.5842 sec.\n",
      "iter 1230 || Loss: 7.4450 || 23:23:46 PM\n",
      "\n",
      "\n",
      "timer: 0.6949 sec.\n",
      "iter 1240 || Loss: 9.2160 || 23:23:53 PM\n",
      "\n",
      "\n",
      "timer: 0.4390 sec.\n",
      "iter 1250 || Loss: 7.9799 || 23:23:59 PM\n",
      "\n",
      "\n",
      "timer: 0.5966 sec.\n",
      "iter 1260 || Loss: 8.0385 || 23:24:10 PM\n",
      "\n",
      "\n",
      "timer: 0.5964 sec.\n",
      "iter 1270 || Loss: 8.4884 || 23:24:17 PM\n",
      "\n",
      "\n",
      "timer: 0.6945 sec.\n",
      "iter 1280 || Loss: 7.7263 || 23:24:24 PM\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timer: 0.6880 sec.\n",
      "iter 1290 || Loss: 9.5053 || 23:24:32 PM\n",
      "\n",
      "\n",
      "timer: 0.5960 sec.\n",
      "iter 1300 || Loss: 10.7347 || 23:24:39 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1300\n",
      "timer: 0.6965 sec.\n",
      "iter 1310 || Loss: 8.0840 || 23:24:47 PM\n",
      "\n",
      "\n",
      "timer: 0.5980 sec.\n",
      "iter 1320 || Loss: 8.5332 || 23:24:53 PM\n",
      "\n",
      "\n",
      "timer: 0.5954 sec.\n",
      "iter 1330 || Loss: 8.3808 || 23:25:01 PM\n",
      "\n",
      "\n",
      "timer: 0.5788 sec.\n",
      "iter 1340 || Loss: 8.6044 || 23:25:08 PM\n",
      "\n",
      "\n",
      "timer: 0.5961 sec.\n",
      "iter 1350 || Loss: 7.8561 || 23:25:16 PM\n",
      "\n",
      "\n",
      "timer: 0.6957 sec.\n",
      "iter 1360 || Loss: 7.7970 || 23:25:23 PM\n",
      "\n",
      "\n",
      "timer: 0.6771 sec.\n",
      "iter 1370 || Loss: 8.2621 || 23:25:31 PM\n",
      "\n",
      "\n",
      "timer: 0.7959 sec.\n",
      "iter 1380 || Loss: 8.3572 || 23:25:38 PM\n",
      "\n",
      "\n",
      "timer: 0.5823 sec.\n",
      "iter 1390 || Loss: 8.8535 || 23:25:46 PM\n",
      "\n",
      "\n",
      "timer: 0.5966 sec.\n",
      "iter 1400 || Loss: 8.6944 || 23:25:52 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1400\n",
      "timer: 0.6814 sec.\n",
      "iter 1410 || Loss: 8.1969 || 23:26:00 PM\n",
      "\n",
      "\n",
      "timer: 0.5956 sec.\n",
      "iter 1420 || Loss: 7.9709 || 23:26:07 PM\n",
      "\n",
      "\n",
      "timer: 0.4926 sec.\n",
      "iter 1430 || Loss: 8.2410 || 23:26:14 PM\n",
      "\n",
      "\n",
      "timer: 0.5951 sec.\n",
      "iter 1440 || Loss: 7.8147 || 23:26:24 PM\n",
      "\n",
      "\n",
      "timer: 0.6958 sec.\n",
      "iter 1450 || Loss: 8.1092 || 23:26:32 PM\n",
      "\n",
      "\n",
      "timer: 0.7761 sec.\n",
      "iter 1460 || Loss: 8.4878 || 23:26:40 PM\n",
      "\n",
      "\n",
      "timer: 0.5964 sec.\n",
      "iter 1470 || Loss: 8.6346 || 23:26:47 PM\n",
      "\n",
      "\n",
      "timer: 0.6866 sec.\n",
      "iter 1480 || Loss: 7.2360 || 23:26:55 PM\n",
      "\n",
      "\n",
      "timer: 0.5912 sec.\n",
      "iter 1490 || Loss: 7.9253 || 23:27:02 PM\n",
      "\n",
      "\n",
      "timer: 0.7884 sec.\n",
      "iter 1500 || Loss: 7.4793 || 23:27:09 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1500\n",
      "timer: 0.6955 sec.\n",
      "iter 1510 || Loss: 9.0035 || 23:27:17 PM\n",
      "\n",
      "\n",
      "timer: 0.7959 sec.\n",
      "iter 1520 || Loss: 7.9362 || 23:27:23 PM\n",
      "\n",
      "\n",
      "timer: 0.5967 sec.\n",
      "iter 1530 || Loss: 7.5567 || 23:27:30 PM\n",
      "\n",
      "\n",
      "timer: 0.6965 sec.\n",
      "iter 1540 || Loss: 23.5574 || 23:27:38 PM\n",
      "\n",
      "\n",
      "timer: 0.5964 sec.\n",
      "iter 1550 || Loss: 15.8969 || 23:27:45 PM\n",
      "\n",
      "\n",
      "timer: 0.5072 sec.\n",
      "iter 1560 || Loss: 51.2297 || 23:27:52 PM\n",
      "\n",
      "\n",
      "timer: 0.5964 sec.\n",
      "iter 1570 || Loss: 8.5777 || 23:28:00 PM\n",
      "\n",
      "\n",
      "timer: 0.5966 sec.\n",
      "iter 1580 || Loss: 8.9005 || 23:28:07 PM\n",
      "\n",
      "\n",
      "timer: 0.6965 sec.\n",
      "iter 1590 || Loss: 8.9692 || 23:28:15 PM\n",
      "\n",
      "\n",
      "timer: 0.5910 sec.\n",
      "iter 1600 || Loss: 8.1126 || 23:28:22 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1600\n",
      "timer: 0.3224 sec.\n",
      "iter 1610 || Loss: 8.0056 || 23:28:28 PM\n",
      "\n",
      "\n",
      "timer: 0.6961 sec.\n",
      "iter 1620 || Loss: 8.1471 || 23:28:39 PM\n",
      "\n",
      "\n",
      "timer: 0.6962 sec.\n",
      "iter 1630 || Loss: 7.5123 || 23:28:46 PM\n",
      "\n",
      "\n",
      "timer: 0.7047 sec.\n",
      "iter 1640 || Loss: 8.3603 || 23:28:54 PM\n",
      "\n",
      "\n",
      "timer: 0.6959 sec.\n",
      "iter 1650 || Loss: 7.4842 || 23:29:01 PM\n",
      "\n",
      "\n",
      "timer: 0.5721 sec.\n",
      "iter 1660 || Loss: 8.2113 || 23:29:08 PM\n",
      "\n",
      "\n",
      "timer: 0.6963 sec.\n",
      "iter 1670 || Loss: 9.4564 || 23:29:16 PM\n",
      "\n",
      "\n",
      "timer: 0.5987 sec.\n",
      "iter 1680 || Loss: 8.5769 || 23:29:23 PM\n",
      "\n",
      "\n",
      "timer: 0.7958 sec.\n",
      "iter 1690 || Loss: 9.5513 || 23:29:31 PM\n",
      "\n",
      "\n",
      "timer: 0.6945 sec.\n",
      "iter 1700 || Loss: 7.6150 || 23:29:38 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1700\n",
      "timer: 0.5354 sec.\n",
      "iter 1710 || Loss: 7.5853 || 23:29:46 PM\n",
      "\n",
      "\n",
      "timer: 0.5991 sec.\n",
      "iter 1720 || Loss: 7.5670 || 23:29:53 PM\n",
      "\n",
      "\n",
      "timer: 0.6964 sec.\n",
      "iter 1730 || Loss: 26.8133 || 23:30:01 PM\n",
      "\n",
      "\n",
      "timer: 0.5897 sec.\n",
      "iter 1740 || Loss: 9.9644 || 23:30:09 PM\n",
      "\n",
      "\n",
      "timer: 0.7885 sec.\n",
      "iter 1750 || Loss: 9.2738 || 23:30:17 PM\n",
      "\n",
      "\n",
      "timer: 0.5808 sec.\n",
      "iter 1760 || Loss: 9.2308 || 23:30:25 PM\n",
      "\n",
      "\n",
      "timer: 0.5956 sec.\n",
      "iter 1770 || Loss: 8.8476 || 23:30:32 PM\n",
      "\n",
      "\n",
      "timer: 0.5821 sec.\n",
      "iter 1780 || Loss: 9.2751 || 23:30:40 PM\n",
      "\n",
      "\n",
      "timer: 0.8994 sec.\n",
      "iter 1790 || Loss: 7.6059 || 23:30:49 PM\n",
      "\n",
      "\n",
      "timer: 0.6945 sec.\n",
      "iter 1800 || Loss: 8.2067 || 23:30:56 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1800\n",
      "timer: 0.5742 sec.\n",
      "iter 1810 || Loss: 10.6124 || 23:31:04 PM\n",
      "\n",
      "\n",
      "timer: 0.5962 sec.\n",
      "iter 1820 || Loss: 8.9361 || 23:31:12 PM\n",
      "\n",
      "\n",
      "timer: 0.7812 sec.\n",
      "iter 1830 || Loss: 8.0889 || 23:31:21 PM\n",
      "\n",
      "\n",
      "timer: 0.6870 sec.\n",
      "iter 1840 || Loss: 8.0098 || 23:31:28 PM\n",
      "\n",
      "\n",
      "timer: 0.6770 sec.\n",
      "iter 1850 || Loss: 7.8700 || 23:31:36 PM\n",
      "\n",
      "\n",
      "timer: 0.5845 sec.\n",
      "iter 1860 || Loss: 9.3419 || 23:31:44 PM\n",
      "\n",
      "\n",
      "timer: 0.6773 sec.\n",
      "iter 1870 || Loss: 8.5927 || 23:31:52 PM\n",
      "\n",
      "\n",
      "timer: 0.6599 sec.\n",
      "iter 1880 || Loss: 8.9982 || 23:32:00 PM\n",
      "\n",
      "\n",
      "timer: 0.7686 sec.\n",
      "iter 1890 || Loss: 8.2681 || 23:32:08 PM\n",
      "\n",
      "\n",
      "timer: 0.4965 sec.\n",
      "iter 1900 || Loss: 7.2789 || 23:32:15 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1900\n",
      "timer: 0.6877 sec.\n",
      "iter 1910 || Loss: 8.6265 || 23:32:23 PM\n",
      "\n",
      "\n",
      "timer: 0.5963 sec.\n",
      "iter 1920 || Loss: 8.9557 || 23:32:30 PM\n",
      "\n",
      "\n",
      "timer: 0.5995 sec.\n",
      "iter 1930 || Loss: 8.6615 || 23:32:38 PM\n",
      "\n",
      "\n",
      "timer: 0.6959 sec.\n",
      "iter 1940 || Loss: 7.6027 || 23:32:46 PM\n",
      "\n",
      "\n",
      "timer: 0.5684 sec.\n",
      "iter 1950 || Loss: 7.2452 || 23:32:53 PM\n",
      "\n",
      "\n",
      "timer: 0.6950 sec.\n",
      "iter 1960 || Loss: 7.8179 || 23:33:00 PM\n",
      "\n",
      "\n",
      "timer: 0.5962 sec.\n",
      "iter 1970 || Loss: 8.5300 || 23:33:10 PM\n",
      "\n",
      "\n",
      "timer: 0.7963 sec.\n",
      "iter 1980 || Loss: 8.0080 || 23:33:17 PM\n",
      "\n",
      "\n",
      "timer: 0.6959 sec.\n",
      "iter 1990 || Loss: 8.7335 || 23:33:24 PM\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train(device, resume)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1000 iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Loss')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open('trained_weights/vgg16_ssd_stats_RMSprop.pkl','rb') as f:\n",
    "    l_loss, c_loss, itr = pickle.load(f)\n",
    "\n",
    "l_loss = np.asarray(l_loss)\n",
    "c_loss = np.asarray(c_loss)\n",
    "itr = np.asarray(itr)\n",
    "plt.plot(itr,l_loss+c_loss)\n",
    "plt.title('Loss - Iterations: lr=1e-3  RMS prop Optimizer')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
