{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import os.path as osp\n",
    "import sys\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import *\n",
    "from utils.augmentations import SSDAugmentation\n",
    "from layers.modules import MultiBoxLoss\n",
    "from ssd import build_ssd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torchvision as tv\n",
    "import torch.optim as optim\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "import torchvision.datasets\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import argparse\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import cv2 \n",
    "import pickle as pkl\n",
    "import random\n",
    "import tarfile\n",
    "import collections\n",
    "import math\n",
    "import datetime\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from config_vgg16_ssd import *\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def xavier(param):\n",
    "    nn.init.xavier_uniform(param)\n",
    "\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        xavier(m.weight.data)\n",
    "        m.bias.data.zero_()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(device, resume=None):\n",
    "    \n",
    "    #use voc config \n",
    "    cfg = voc\n",
    " \n",
    "    ssd_net = build_ssd('train', cfg['min_dim'], cfg['num_classes'])\n",
    "    dataset = VOCDetection(root=dataset_root, image_sets=[('2012', 'train')],\n",
    "                            transform=SSDAugmentation(cfg['min_dim'],\n",
    "                             MEANS))\n",
    " \n",
    "\n",
    "    if resume:\n",
    "        print('Resuming training, loading previous training at ',resume)\n",
    "        ssd_net.load_weights(resume) \n",
    "    else:\n",
    "        vgg_weights = torch.load(basenet)\n",
    "        print('Loading base network...')\n",
    "        ssd_net.vgg.load_state_dict(vgg_weights)\n",
    "        print('Initializing weights...')\n",
    "        ssd_net.extras.apply(weights_init)\n",
    "        ssd_net.loc.apply(weights_init)\n",
    "        ssd_net.conf.apply(weights_init)\n",
    "    \n",
    "    net = ssd_net\n",
    "    \n",
    "    if device:\n",
    "        net = torch.nn.DataParallel(ssd_net)\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        net = net.to(device)\n",
    " \n",
    "    optimizer = optim.Adam(net.parameters(), lr=lr,weight_decay=weight_decay)  \n",
    "    \n",
    "    criterion = MultiBoxLoss(cfg['num_classes'], 0.5, True, 0, True, 3, 0.5,\n",
    "                             False, torch.cuda.is_available())\n",
    "\n",
    "    \n",
    "    net.train()\n",
    "    mode = 'train'\n",
    "    \n",
    "\n",
    "    loc_loss = 0\n",
    "    conf_loss = 0\n",
    "    epoch = 0\n",
    "    print('Loading the dataset...')\n",
    "\n",
    "    epoch_size = len(dataset) // batch_size\n",
    "    print('Training SSD on: ',mode)\n",
    "\n",
    "    step_index = 0\n",
    "\n",
    "    #based on adapted code\n",
    "    train_data_loader = data.DataLoader(dataset, batch_size=32, num_workers=4, shuffle=True,\\\n",
    "                               collate_fn=detection_collate,pin_memory=True)\n",
    "    \n",
    "    print(\"Images in the training set = \" + str(len(dataset)))\n",
    "    print(\"Images in a mini-batch = \"+str(batch_size))\n",
    "    print(\"mini-batches = \" + str(len(train_data_loader)))\n",
    "    \n",
    "    \n",
    "     # create batch iterator\n",
    "    batch_iterator = iter(train_data_loader)\n",
    "    print(\"STARTING - ITERATIONS\")\n",
    "    \n",
    "    \n",
    "    l_loss = []\n",
    "    c_loss = []\n",
    "    itr = []\n",
    "    \n",
    "    #for 10000 iterations - takes long\n",
    "    for iteration in range(0, 10000):\n",
    "        \n",
    "        if iteration != 0 and (iteration % epoch_size == 0):\n",
    "            # reset epoch loss counters\n",
    "            loc_loss = 0\n",
    "            conf_loss = 0\n",
    "            epoch += 1\n",
    "\n",
    "        if iteration in cfg['lr_steps']:\n",
    "            step_index += 1\n",
    "            lr_dec = lr * (gamma ** (step_index))\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr_dec\n",
    "\n",
    "            \n",
    "        ## load train data\n",
    "        try:\n",
    "            images, targets = next(batch_iterator)\n",
    "        except StopIteration:\n",
    "            batch_iterator = iter(train_data_loader)\n",
    "            images, targets = next(batch_iterator)\n",
    "\n",
    "\n",
    "        \n",
    "        if device:\n",
    "            images = images.cuda()\n",
    "            targets = [ann.cuda() for ann in targets]\n",
    "        else:\n",
    "            images = images\n",
    "            targets = [ann for ann in targets]\n",
    "        \n",
    "        # forward\n",
    "        t0 = time.time()\n",
    "        out = net(images)\n",
    "\n",
    "        # backprop\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss_l, loss_c = criterion(out, targets)\n",
    "        loss = loss_l + loss_c\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        l_loss.append(loss_l.data.item())\n",
    "        c_loss.append(loss_c.data.item())\n",
    "        \n",
    "        t1 = time.time()\n",
    "        loc_loss += loss_l.data.item()\n",
    "        conf_loss += loss_c.data.item()\n",
    "        \n",
    "        itr.append(iteration)\n",
    "        \n",
    "        if iteration % 10 == 0:\n",
    "            print('timer: %.4f sec.' % (t1 - t0))\n",
    "            print('iter ' + repr(iteration) + ' || Loss: %.4f ||' % (loss.data.item()), end=' ')\n",
    "            currentDT = datetime.datetime.now()\n",
    "            print (currentDT.strftime(\"%H:%M:%S %p\"))\n",
    "            print(\"\\n\")\n",
    "        \n",
    "        \n",
    "        if iteration != 0 and iteration % 100 == 0:\n",
    "            print('Saving state, iter:', iteration)\n",
    "            iter_name = math.ceil(iteration/100)*100\n",
    "            torch.save(ssd_net.state_dict(), 'trained_weights/ssd_VOC_adam_' +repr(iter_name) + '.pth')\n",
    "            with open('trained_weights/vgg16_ssd_stats_adam.pkl','wb') as f:\n",
    "                pkl.dump([l_loss, c_loss, itr], f)\n",
    "                \n",
    "\n",
    "    torch.save(ssd_net.state_dict(),\n",
    "               save_folder + data_set + '.pth') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Initialize pointers\r\n",
      "data_set = 'VOC'\r\n",
      "dataset_root = voc_root = '//datasets/ee285f-public/PascalVOC2012/'\r\n",
      "save_folder = 'trained_weights/'\r\n",
      "eval_save_folder = 'eval/'\r\n",
      "devkit_path = 'devkit_path/'\r\n",
      "output_dir = \"out/\"\r\n",
      "\r\n",
      "#Run related metaparameters\r\n",
      "\r\n",
      "batch_size = 32\r\n",
      "resume = None\r\n",
      "\r\n",
      "#Optimization metaparameters\r\n",
      "lr = 1e-3\r\n",
      "momentum = 0.9\r\n",
      "weight_decay = 5e-4\r\n",
      "gamma = 0.1\r\n",
      "    \r\n",
      "confidence_threshold = 0.01\r\n",
      "top_k = 5\r\n",
      "cleanup = True\r\n",
      "\r\n",
      "YEAR = '2012'\r\n",
      "dataset_mean = (104, 117, 123)\r\n",
      "set_type = 'train'\r\n",
      "\r\n",
      "# Please Change if required\r\n",
      "trained_model = 'weights/ssd_pretrained.pth'\r\n",
      "basenet = 'weights/vgg16_reducedfc.pth'"
     ]
    }
   ],
   "source": [
    "!cat config_vgg16_ssd.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "change log in ssd.py\n",
      "Loading base network...\n",
      "Initializing weights...\n",
      "Loading the dataset...\n",
      "Training SSD on:  train\n",
      "Images in the training set = 5717\n",
      "Images in a mini-batch = 32\n",
      "mini-batches = 179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:2: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING - ITERATIONS\n",
      "timer: 7.2073 sec.\n",
      "iter 0 || Loss: 25.5797 || 19:35:09 PM\n",
      "\n",
      "\n",
      "timer: 0.6409 sec.\n",
      "iter 10 || Loss: 15.1535 || 19:35:15 PM\n",
      "\n",
      "\n",
      "timer: 0.6964 sec.\n",
      "iter 20 || Loss: 11.0326 || 19:35:22 PM\n",
      "\n",
      "\n",
      "timer: 0.6839 sec.\n",
      "iter 30 || Loss: 8.7313 || 19:35:29 PM\n",
      "\n",
      "\n",
      "timer: 0.6965 sec.\n",
      "iter 40 || Loss: 8.0529 || 19:35:37 PM\n",
      "\n",
      "\n",
      "timer: 0.9787 sec.\n",
      "iter 50 || Loss: 8.2705 || 19:35:44 PM\n",
      "\n",
      "\n",
      "timer: 0.7048 sec.\n",
      "iter 60 || Loss: 8.1091 || 19:35:51 PM\n",
      "\n",
      "\n",
      "timer: 0.7797 sec.\n",
      "iter 70 || Loss: 7.6529 || 19:35:59 PM\n",
      "\n",
      "\n",
      "timer: 0.6954 sec.\n",
      "iter 80 || Loss: 8.0486 || 19:36:06 PM\n",
      "\n",
      "\n",
      "timer: 0.5951 sec.\n",
      "iter 90 || Loss: 7.2620 || 19:36:13 PM\n",
      "\n",
      "\n",
      "timer: 0.5933 sec.\n",
      "iter 100 || Loss: 7.8164 || 19:36:21 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 100\n",
      "timer: 0.6954 sec.\n",
      "iter 110 || Loss: 7.0772 || 19:36:28 PM\n",
      "\n",
      "\n",
      "timer: 0.6945 sec.\n",
      "iter 120 || Loss: 7.7748 || 19:36:36 PM\n",
      "\n",
      "\n",
      "timer: 0.5963 sec.\n",
      "iter 130 || Loss: 7.5215 || 19:36:43 PM\n",
      "\n",
      "\n",
      "timer: 0.7954 sec.\n",
      "iter 140 || Loss: 7.1587 || 19:36:50 PM\n",
      "\n",
      "\n",
      "timer: 0.5959 sec.\n",
      "iter 150 || Loss: 7.6166 || 19:36:57 PM\n",
      "\n",
      "\n",
      "timer: 0.6961 sec.\n",
      "iter 160 || Loss: 7.5524 || 19:37:04 PM\n",
      "\n",
      "\n",
      "timer: 0.6903 sec.\n",
      "iter 170 || Loss: 7.6351 || 19:37:11 PM\n",
      "\n",
      "\n",
      "timer: 0.6924 sec.\n",
      "iter 180 || Loss: 7.8011 || 19:37:24 PM\n",
      "\n",
      "\n",
      "timer: 0.6963 sec.\n",
      "iter 190 || Loss: 7.7705 || 19:37:31 PM\n",
      "\n",
      "\n",
      "timer: 0.6964 sec.\n",
      "iter 200 || Loss: 7.2594 || 19:37:39 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 200\n",
      "timer: 0.5965 sec.\n",
      "iter 210 || Loss: 7.5832 || 19:37:46 PM\n",
      "\n",
      "\n",
      "timer: 0.5964 sec.\n",
      "iter 220 || Loss: 7.5075 || 19:37:53 PM\n",
      "\n",
      "\n",
      "timer: 0.6960 sec.\n",
      "iter 230 || Loss: 7.1937 || 19:38:00 PM\n",
      "\n",
      "\n",
      "timer: 0.6946 sec.\n",
      "iter 240 || Loss: 7.5812 || 19:38:07 PM\n",
      "\n",
      "\n",
      "timer: 0.6986 sec.\n",
      "iter 250 || Loss: 7.2454 || 19:38:14 PM\n",
      "\n",
      "\n",
      "timer: 0.6852 sec.\n",
      "iter 260 || Loss: 7.4630 || 19:38:21 PM\n",
      "\n",
      "\n",
      "timer: 0.7963 sec.\n",
      "iter 270 || Loss: 7.5851 || 19:38:28 PM\n",
      "\n",
      "\n",
      "timer: 0.4959 sec.\n",
      "iter 280 || Loss: 7.0728 || 19:38:35 PM\n",
      "\n",
      "\n",
      "timer: 0.6831 sec.\n",
      "iter 290 || Loss: 7.6873 || 19:38:42 PM\n",
      "\n",
      "\n",
      "timer: 0.5963 sec.\n",
      "iter 300 || Loss: 7.8093 || 19:38:49 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 300\n",
      "timer: 0.5887 sec.\n",
      "iter 310 || Loss: 7.3654 || 19:38:57 PM\n",
      "\n",
      "\n",
      "timer: 0.5948 sec.\n",
      "iter 320 || Loss: 7.2148 || 19:39:03 PM\n",
      "\n",
      "\n",
      "timer: 0.6962 sec.\n",
      "iter 330 || Loss: 7.2117 || 19:39:10 PM\n",
      "\n",
      "\n",
      "timer: 0.5969 sec.\n",
      "iter 340 || Loss: 7.5433 || 19:39:17 PM\n",
      "\n",
      "\n",
      "timer: 0.5794 sec.\n",
      "iter 350 || Loss: 7.1740 || 19:39:23 PM\n",
      "\n",
      "\n",
      "timer: 0.7081 sec.\n",
      "iter 360 || Loss: 7.3243 || 19:39:32 PM\n",
      "\n",
      "\n",
      "timer: 0.6964 sec.\n",
      "iter 370 || Loss: 7.4604 || 19:39:39 PM\n",
      "\n",
      "\n",
      "timer: 0.6965 sec.\n",
      "iter 380 || Loss: 6.8660 || 19:39:46 PM\n",
      "\n",
      "\n",
      "timer: 0.5961 sec.\n",
      "iter 390 || Loss: 6.9310 || 19:39:53 PM\n",
      "\n",
      "\n",
      "timer: 0.5965 sec.\n",
      "iter 400 || Loss: 7.2215 || 19:40:00 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 400\n",
      "timer: 0.6956 sec.\n",
      "iter 410 || Loss: 7.1266 || 19:40:07 PM\n",
      "\n",
      "\n",
      "timer: 0.5917 sec.\n",
      "iter 420 || Loss: 7.1504 || 19:40:14 PM\n",
      "\n",
      "\n",
      "timer: 0.6969 sec.\n",
      "iter 430 || Loss: 7.2196 || 19:40:21 PM\n",
      "\n",
      "\n",
      "timer: 0.6939 sec.\n",
      "iter 440 || Loss: 7.3163 || 19:40:28 PM\n",
      "\n",
      "\n",
      "timer: 0.7185 sec.\n",
      "iter 450 || Loss: 7.2000 || 19:40:35 PM\n",
      "\n",
      "\n",
      "timer: 0.5961 sec.\n",
      "iter 460 || Loss: 7.3342 || 19:40:42 PM\n",
      "\n",
      "\n",
      "timer: 0.6939 sec.\n",
      "iter 470 || Loss: 7.3286 || 19:40:49 PM\n",
      "\n",
      "\n",
      "timer: 0.6147 sec.\n",
      "iter 480 || Loss: 7.1433 || 19:40:55 PM\n",
      "\n",
      "\n",
      "timer: 0.5961 sec.\n",
      "iter 490 || Loss: 6.6970 || 19:41:02 PM\n",
      "\n",
      "\n",
      "timer: 0.6956 sec.\n",
      "iter 500 || Loss: 7.1250 || 19:41:09 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 500\n",
      "timer: 0.6803 sec.\n",
      "iter 510 || Loss: 6.6491 || 19:41:17 PM\n",
      "\n",
      "\n",
      "timer: 0.6954 sec.\n",
      "iter 520 || Loss: 7.0566 || 19:41:24 PM\n",
      "\n",
      "\n",
      "timer: 0.4490 sec.\n",
      "iter 530 || Loss: 7.1743 || 19:41:31 PM\n",
      "\n",
      "\n",
      "timer: 0.6143 sec.\n",
      "iter 540 || Loss: 7.2723 || 19:41:39 PM\n",
      "\n",
      "\n",
      "timer: 0.7271 sec.\n",
      "iter 550 || Loss: 6.9795 || 19:41:47 PM\n",
      "\n",
      "\n",
      "timer: 0.6962 sec.\n",
      "iter 560 || Loss: 6.9936 || 19:41:53 PM\n",
      "\n",
      "\n",
      "timer: 0.5963 sec.\n",
      "iter 570 || Loss: 7.2653 || 19:42:00 PM\n",
      "\n",
      "\n",
      "timer: 0.7953 sec.\n",
      "iter 580 || Loss: 7.2113 || 19:42:07 PM\n",
      "\n",
      "\n",
      "timer: 0.5961 sec.\n",
      "iter 590 || Loss: 6.9255 || 19:42:14 PM\n",
      "\n",
      "\n",
      "timer: 0.6962 sec.\n",
      "iter 600 || Loss: 7.2372 || 19:42:21 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 600\n",
      "timer: 0.6954 sec.\n",
      "iter 610 || Loss: 7.2688 || 19:42:28 PM\n",
      "\n",
      "\n",
      "timer: 0.5178 sec.\n",
      "iter 620 || Loss: 6.9655 || 19:42:35 PM\n",
      "\n",
      "\n",
      "timer: 0.5963 sec.\n",
      "iter 630 || Loss: 6.9057 || 19:42:42 PM\n",
      "\n",
      "\n",
      "timer: 0.6962 sec.\n",
      "iter 640 || Loss: 6.9217 || 19:42:48 PM\n",
      "\n",
      "\n",
      "timer: 0.6952 sec.\n",
      "iter 650 || Loss: 6.9124 || 19:42:55 PM\n",
      "\n",
      "\n",
      "timer: 0.5962 sec.\n",
      "iter 660 || Loss: 6.6461 || 19:43:01 PM\n",
      "\n",
      "\n",
      "timer: 0.6001 sec.\n",
      "iter 670 || Loss: 7.2395 || 19:43:09 PM\n",
      "\n",
      "\n",
      "timer: 0.6891 sec.\n",
      "iter 680 || Loss: 7.3318 || 19:43:16 PM\n",
      "\n",
      "\n",
      "timer: 0.6957 sec.\n",
      "iter 690 || Loss: 6.5622 || 19:43:23 PM\n",
      "\n",
      "\n",
      "timer: 0.6959 sec.\n",
      "iter 700 || Loss: 7.2485 || 19:43:30 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 700\n",
      "timer: 0.6580 sec.\n",
      "iter 710 || Loss: 6.8843 || 19:43:37 PM\n",
      "\n",
      "\n",
      "timer: 0.6956 sec.\n",
      "iter 720 || Loss: 7.1707 || 19:43:47 PM\n",
      "\n",
      "\n",
      "timer: 0.6847 sec.\n",
      "iter 730 || Loss: 6.7213 || 19:43:54 PM\n",
      "\n",
      "\n",
      "timer: 0.6763 sec.\n",
      "iter 740 || Loss: 7.4764 || 19:44:01 PM\n",
      "\n",
      "\n",
      "timer: 0.6966 sec.\n",
      "iter 750 || Loss: 6.8502 || 19:44:08 PM\n",
      "\n",
      "\n",
      "timer: 0.5958 sec.\n",
      "iter 760 || Loss: 6.5810 || 19:44:15 PM\n",
      "\n",
      "\n",
      "timer: 0.5963 sec.\n",
      "iter 770 || Loss: 6.8310 || 19:44:21 PM\n",
      "\n",
      "\n",
      "timer: 0.6963 sec.\n",
      "iter 780 || Loss: 7.1878 || 19:44:28 PM\n",
      "\n",
      "\n",
      "timer: 0.5932 sec.\n",
      "iter 790 || Loss: 7.5158 || 19:44:34 PM\n",
      "\n",
      "\n",
      "timer: 0.5958 sec.\n",
      "iter 800 || Loss: 6.8894 || 19:44:41 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 800\n",
      "timer: 0.6124 sec.\n",
      "iter 810 || Loss: 6.7678 || 19:44:48 PM\n",
      "\n",
      "\n",
      "timer: 0.7957 sec.\n",
      "iter 820 || Loss: 6.7081 || 19:44:55 PM\n",
      "\n",
      "\n",
      "timer: 0.6960 sec.\n",
      "iter 830 || Loss: 6.4018 || 19:45:02 PM\n",
      "\n",
      "\n",
      "timer: 0.7959 sec.\n",
      "iter 840 || Loss: 6.6884 || 19:45:11 PM\n",
      "\n",
      "\n",
      "timer: 0.7991 sec.\n",
      "iter 850 || Loss: 6.8324 || 19:45:18 PM\n",
      "\n",
      "\n",
      "timer: 0.6967 sec.\n",
      "iter 860 || Loss: 6.5960 || 19:45:26 PM\n",
      "\n",
      "\n",
      "timer: 0.5962 sec.\n",
      "iter 870 || Loss: 7.0649 || 19:45:35 PM\n",
      "\n",
      "\n",
      "timer: 0.6178 sec.\n",
      "iter 880 || Loss: 6.8354 || 19:45:43 PM\n",
      "\n",
      "\n",
      "timer: 0.4631 sec.\n",
      "iter 890 || Loss: 6.8078 || 19:45:51 PM\n",
      "\n",
      "\n",
      "timer: 0.6959 sec.\n",
      "iter 900 || Loss: 6.9190 || 19:46:00 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 900\n",
      "timer: 0.6989 sec.\n",
      "iter 910 || Loss: 6.9190 || 19:46:08 PM\n",
      "\n",
      "\n",
      "timer: 0.7963 sec.\n",
      "iter 920 || Loss: 6.7130 || 19:46:15 PM\n",
      "\n",
      "\n",
      "timer: 0.6964 sec.\n",
      "iter 930 || Loss: 6.5408 || 19:46:21 PM\n",
      "\n",
      "\n",
      "timer: 0.5963 sec.\n",
      "iter 940 || Loss: 6.6827 || 19:46:28 PM\n",
      "\n",
      "\n",
      "timer: 0.4974 sec.\n",
      "iter 950 || Loss: 6.7613 || 19:46:35 PM\n",
      "\n",
      "\n",
      "timer: 0.7176 sec.\n",
      "iter 960 || Loss: 6.7863 || 19:46:42 PM\n",
      "\n",
      "\n",
      "timer: 0.5961 sec.\n",
      "iter 970 || Loss: 6.6580 || 19:46:48 PM\n",
      "\n",
      "\n",
      "timer: 0.5906 sec.\n",
      "iter 980 || Loss: 7.1027 || 19:46:55 PM\n",
      "\n",
      "\n",
      "timer: 0.5962 sec.\n",
      "iter 990 || Loss: 6.8771 || 19:47:02 PM\n",
      "\n",
      "\n",
      "timer: 0.6874 sec.\n",
      "iter 1000 || Loss: 6.6340 || 19:47:09 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 1000\n",
      "timer: 0.5955 sec.\n",
      "iter 1010 || Loss: 7.3549 || 19:47:16 PM\n",
      "\n",
      "\n",
      "timer: 0.6729 sec.\n",
      "iter 1020 || Loss: 6.8333 || 19:47:24 PM\n",
      "\n",
      "\n",
      "timer: 0.6983 sec.\n",
      "iter 1030 || Loss: 6.7207 || 19:47:31 PM\n",
      "\n",
      "\n",
      "timer: 0.5858 sec.\n",
      "iter 1040 || Loss: 7.0384 || 19:47:38 PM\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-e0b89bcb91b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresume\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-0472e637dc21>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(device, resume)\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0mloss_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_l\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_c\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mlip_proj/final/ssd-master_qfgaohao/layers/modules/multibox_loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, predictions, targets)\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0mdefaults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpriors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             match(self.threshold, truths, defaults, self.variance, labels,\n\u001b[0;32m---> 73\u001b[0;31m                   loc_t, conf_t, idx)\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_gpu\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mloc_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloc_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mlip_proj/final/ssd-master_qfgaohao/layers/box_utils.py\u001b[0m in \u001b[0;36mmatch\u001b[0;34m(threshold, truths, priors, variances, labels, loc_t, conf_t, idx)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;31m# ensure every gt matches with its prior of max overlap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_prior_idx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0mbest_truth_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbest_prior_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m     \u001b[0mmatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtruths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbest_truth_idx\u001b[0m\u001b[0;34m]\u001b[0m          \u001b[0;31m# Shape: [num_priors,4]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0mconf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbest_truth_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m         \u001b[0;31m# Shape: [num_priors]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(device, resume)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1000 iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Loss')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3de5xcdX3/8dd7Z2cv2exuErIJuWECRCBQQYwpVP2VohbEC3iHitJWf2jrvbUW6K8ttdIf/dVLa622KCq/nyBS8UL5eUcR8adgwAAJEYm5mE3CZnPd+23m8/vjnJ1MNpPN7iazm+y8n4/HPObMd8458/memTmf+X6/Z85RRGBmZgZQNdUBmJnZ8cNJwczMCpwUzMyswEnBzMwKnBTMzKzAScHMzAqcFGxKSXqRpKemOIb7Jb1tKmOYapI2S3rJVMcxUZL+XdJfT3DZGyR99ljHdKJyUpgEx+sXTtJSSSGpOn38BUkfLvNrhqTThx9HxI8j4oxyvuZkkbRA0j2Stqf1XHqU6/uhpHZJHZIek3T5sYl0aklaLOl2SbsldUt6WNIrxrH8H0p6sLgsIt4REX8/kXgi4h8ioqJ/FBRzUrBjZji5TCfjrFMe+Dbw2mP08u8FFkREE3At8EVJC47RuqeEpDnAg8AAcDYwF/g4cIek101lbMfSCf1diAjfynwDNgMvOcxz/x3YAOwB7gEWpuUi+bLsBPYDjwPnpM9dBjwJdALbgA9MMK6lQADVJDudQZIvaxfwX+k8C4G7gXZgE/CeouVvBL4CfBHoAN4GrAJ+CuwDdgCfBGrS+R9IX687fY03AhcBrUXrPAu4P11+HfCqoue+APwb8H/Tuj8EnHak7TWG7XA/8LZ0+g+Bn6Tr2gN8eALbtTqt59IR5c3Arel22QZ8GMiMcZ2rgD5g1Rjnfznwi/R92QrcOOL5NwNbgN3AXxV/Rkd7D9PnA/hT4On0ffh74LR0mQ7gruL5R7zu3wNrgaoR5X+ZxqOi13gPsBHYBfwTyY/Ys9LtkEs/Q/uKPhsfTqcvAlqBD6afhx3AFSTfm1+l7+sNIz7HX0ynP5mud/g2NLztGOd3Yar3OxO9TXkAlXDjMEkBuDj9wJ8P1AL/CjyQPncJ8Agwi2SHdxbJr0bSD/mL0unZwPkTjGtp+uWrTh8Xvljp46o0hr8BaoBT0y/pJenzN5IkkivSeeuB5wEXkOwYlwLrgfcVrTOA04seX0SaFIAsSYK8IX29i0l2OmcUxbeHZKdVDdwO3DmG7fUHwOOjbIf7OTgpDAHvTl+jHnghyQ7ycLcXjljf4ZLC14H/ABqAecDDwNuP8B7dS7ITDJJWSNVo84/Yrr+Vvi/PAdqAK9LnVpDs8P4byefuY2mdh5PCWN7De4Amkl/7/cB96eejmeQHyzWHietnwN+VKF+WrveMotf4ITAHOIVkZ178Hj04YvkvcHBSGCL53GZJfni1A3cAjWnMfcCpRZ/jL5aI6bx0uecyge/CVO93Jnpz99HUehPwuYh4NCL6geuBC9O+6EGSD/CZJL+e1kfEjnS5QWCFpKaI2BsRj5YpvucDLRHxoYgYiIiNwGeAK4vm+WlEfD0i8hHRGxGPRMTPImIoIjaT7AR/d4yvdwEwE7g5fb0fkOwUryqa56sR8XBEDJEkhfPS8sNur4i4IyKeM456b4+If03r0BsRD0bErFFuDx5phZLmAy8j2bl2R8ROktbIlaMtFxGvSOt1GfCdiMiPpQIRcX9EPJG+L48DX+LA+/A64N6IeCD93P01SdfX8LJjeQ//MSI6ImIdyS//70bExojYD3yLZEdaylySHzUj7Sh6vvg19kTEb4B/5uDPwZEMAjdFxCBwZ7ref4mIzjTmdSTJsiRJLSRJ/N0R8Qsm8F0YR6zHFSeFqbWQpMkMQER0kTTnF6U7xE+SdJe0SbpFUlM662tJdhJbJP1I0oWlVi5pnaSu9PaiCcT3LGChpH3DN5Jf8fOL5tk64jWfLeleSc9I6gD+gYO/6KNZCGwdsePbAiwqevxM0XQPSRLhCNtrvLYeeZZxexbJr9YdRdvyP0haDKO+VxExGBHfAi6R9KqxvJik3y4aqN4PvIMD78NCiuoYEd0kn7vhZcfyHrYVTfeWeDzzMKHtAkqNiywoen5Y8fuwJY17rHZHRK4oHsYao6QsSVfQHRFxZ1o87u/CicpJYWptJ/mwASCpATiJpL+ZiPhERDyPpLn7bOAv0vKfR8TlJDuUr5P04R4iIs6OiJnp7cdjiGfkKXO3AptG/CpujIjLRlnm08AvgeWRDJDeQNKdMxbbgSWSij+Xp5BujyMGf5jtNQEH1Sk9bLZrlNtYEu5Wkm6WuUXbsikizk5jH8t7VU3Sdz8Wd5B08SyJiGbg3znwPuwAlhTVbwbJ527Y0byHR/J94LUj3mOAN5Bso18VlS0pmj6F5PMBh37mjrV/Jem2/B9FZRP5LpyQnBQmT1ZSXdGtmuSL+0eSzpNUS/KL7KGI2Czp+emvvSzJwGwfkJNUI+lNkprTpnEHyaDbsdBG0lc67GGgQ9JfSqqXlJF0jqTnj7KOxjSmLklnAn9yhNco9hBJXT8oKSvpIuCVJM3/UR1uex1pubGI5LDZmaPcCjtxSXUk/fQAtelj0q6s7wIfldQkqUrSaZJKdq1JOlPSy9LtnpV0NckYwI/GGHYjsCci+iStIhlXGfYV4BWSXiipBvgQB+8LjvQeHo2Pk4xF3Crp5PS7cBXJYPdfRETxjvUvJM2WtITkSKwvp+VtwOI09mNK0ttJusr+YESLdSLfhROSk8Lk+SZJk3X4dmNE3EfSn3s3ya+30zjQR9lE0me5lwNHiXwkfe7NwOa0af8O4OpjFOOtJGMV+yR9PW1+v5Kk334TSdP+sySDiYfzAZIdUGca/5dHPH8jcFv6Gm8ofiIiBoBXkfS97wI+BbwlIn45htgPu73SJLpuDOs4FnpJBnEh+bVd3Lf8FpJByifTOL9C6a4USH6Z30hy9Ew7yU7xjeMYP/pT4EOSOkkGRwutybRP/Z0kP0p2pLG0Fi17pPdwwiJiN8nAfR3JdtgN/Bnw5ogY+TrfIBncXUNyxNmtafkPSMYEnpG0i2PrKpIfLduLWoI3TPC7cELSwYnZzGzqSQqS7qsNUx1LpXFLwczMCpwUzMyswN1HZmZW4JaCmZkVnLgnbQLmzp0bS5cuneowzMxOKI888siuiGgp9dwJnRSWLl3K6tWrpzoMM7MTiqQth3vO3UdmZlZQtqSQ/lPxYSUXB1kn6e/S8jmSvifp6fR+dtEy10vaIOkpSZeUKzYzMyutnC2FfuDiiDiX5F+Al0q6ALgOuC8ilpOcbvc6AEkrSP7NezZwKfApSZkyxmdmZiOULSlEYvjv/tn0FsDlwG1p+W0k5x8nLb8zIvojYhPJefVXlSs+MzM7VFnHFNKTRq0hOX/L9yLiIWB+0Xnud5CeOpjk9MjFp55t5eBTJg+v81pJqyWtbm9vL2f4ZmYVp6xJISJyEXEesBhYJemcUWYvdWreQ/5ZFxG3RMTKiFjZ0lLyiCozM5ugSTn6KCL2kVzy8FKSC6AsAEjvd6aztXLw+dMXc+D86WZmNgnKefRRi6RZ6XQ98BKSUwnfA1yTznYNyelxScuvlFQraRmwnOQc5mXx4NO72Lyru1yrNzM7IZXzz2sLSM6bnyFJPndFxL2SfgrcJemtwG+A10NyjndJd5GcY30IeGfR5fSOuatvfQiAzTe/vFwvYWZ2wilbUkgvFn7IxbvTi2y8+DDL3ATcVK6YzMxsdP5Hs5mZFTgpmJlZgZOCmZkVOCmYmVmBk4KZmRU4KZiZWYGTgpmZFTgpmJlZgZOCmZkVOCmYmVmBk4KZmRU4KZiZWYGTgpmZFTgpmJlZgZOCmZkVOCmYmVmBk4KZmRU4KZiZWYGTgpmZFTgpmJlZgZOCmZkVOCmYmVmBk4KZmRU4KZiZWYGTgpmZFZQtKUhaIumHktZLWifpvWn5jZK2SVqT3i4rWuZ6SRskPSXpknLFZmZmpVWXcd1DwJ9HxKOSGoFHJH0vfe7jEfGR4pklrQCuBM4GFgLfl/TsiMiVMUYzMytStpZCROyIiEfT6U5gPbBolEUuB+6MiP6I2ARsAFaVKz4zMzvUpIwpSFoKPBd4KC16l6THJX1O0uy0bBGwtWixVkokEUnXSlotaXV7e3sZozYzqzxlTwqSZgJ3A++LiA7g08BpwHnADuCjw7OWWDwOKYi4JSJWRsTKlpaWMkVtZlaZypoUJGVJEsLtEfFVgIhoi4hcROSBz3Cgi6gVWFK0+GJgeznjMzOzg5Xz6CMBtwLrI+JjReULimZ7NbA2nb4HuFJSraRlwHLg4XLFZ2Zmhyrn0UcvAN4MPCFpTVp2A3CVpPNIuoY2A28HiIh1ku4CniQ5cumdPvLIzGxylS0pRMSDlB4n+OYoy9wE3FSumMzMbHT+R7OZmRU4KZiZWYGTgpmZFTgpmJlZgZOCmZkVOCmYmVmBk4KZmRU4KZiZWYGTgpmZFTgpmJlZgZOCmZkVOCmYmVmBk4KZmRU4KZiZWYGTgpmZFTgpmJlZgZOCmZkVOCmYmVmBk4KZmRU4KZiZWYGTgpmZFTgpmJlZgZOCmZkVVGRSiIipDsHM7LhUkUnBzMxKq8ik4IaCmVlpZUsKkpZI+qGk9ZLWSXpvWj5H0vckPZ3ezy5a5npJGyQ9JemScsVmZmallbOlMAT8eUScBVwAvFPSCuA64L6IWA7clz4mfe5K4GzgUuBTkjJljM/MzEYoW1KIiB0R8Wg63QmsBxYBlwO3pbPdBlyRTl8O3BkR/RGxCdgArCpLbOVYqZnZNDApYwqSlgLPBR4C5kfEDkgSBzAvnW0RsLVosda0bOS6rpW0WtLq9vb2coZtZlZxyp4UJM0E7gbeFxEdo81aouyQH/URcUtErIyIlS0tLROKyYekmpmVVtakIClLkhBuj4ivpsVtkhakzy8AdqblrcCSosUXA9vLGZ+ZmR2snEcfCbgVWB8RHyt66h7gmnT6GuAbReVXSqqVtAxYDjxcrvjMzOxQ1WVc9wuANwNPSFqTlt0A3AzcJemtwG+A1wNExDpJdwFPkhy59M6IyJUjMHcemZmVVrakEBEPUnqcAODFh1nmJuCmcsVkZmaj8z+azcysoCKTgpmZlVaRSSE8qmBmVlJFJgUzMyvNScHMzAoqMil4oNnMrLSKTApmZlaak4KZmRU4KZiZWUFFJgWPKZiZlVaRScHMzEpzUjAzs4KKTAr+R7OZWWkVmRTMzKy0ikwKHmg2MyutIpOCmZmV5qRgZmYFFZkU3HtkZlZaRSYFMzMrrSKTQnik2cyspDElBUmnSapNpy+S9B5Js8obmpmZTbaxthTuBnKSTgduBZYBd5QtqjJzO8HMrLSxJoV8RAwBrwb+OSLeDywoX1hmZjYVxpoUBiVdBVwD3JuWZcsTkpmZTZWxJoU/Ai4EboqITZKWAV8sX1jl5XFmM7PSxpQUIuLJiHhPRHxJ0mygMSJuHm0ZSZ+TtFPS2qKyGyVtk7QmvV1W9Nz1kjZIekrSJROukZmZTdhYjz66X1KTpDnAY8DnJX3sCIt9Abi0RPnHI+K89PbNdP0rgCuBs9NlPiUpM9ZKjJtbCmZmJY21+6g5IjqA1wCfj4jnAS8ZbYGIeADYM8b1Xw7cGRH9EbEJ2ACsGuOyZmZ2jIw1KVRLWgC8gQMDzRP1LkmPp91Ls9OyRcDWonla07JDSLpW0mpJq9vb2ycUgK+nYGZW2liTwoeA7wC/joifSzoVeHoCr/dp4DTgPGAH8NG0XCXmLbnnjohbImJlRKxsaWmZQAhmZnY41WOZKSL+E/jPoscbgdeO98Uiom14WtJnONDqaAWWFM26GNg+3vWbmdnRGetA82JJX0uPJmqTdLekxeN9sbQLatirgeEjk+4BrpRUmx7uuhx4eLzrHysfkmpmVtqYWgrA50lOa/H69PHVadlLD7eApC8BFwFzJbUCfwtcJOk8kq6hzcDbASJinaS7gCeBIeCdEZEbb2XMzOzojDUptETE54sef0HS+0ZbICKuKlF86yjz3wTcNMZ4joobCmZmpY11oHmXpKslZdLb1cDucgZmZmaTb6xJ4Y9JDkd9huSoodeRnPrCzMymkbGe5uI3EfGqiGiJiHkRcQXJH9lOSL7IjplZaUdz5bU/O2ZRmJnZceFokkKpP5ydENxOMDMr7WiSgvetZmbTzKiHpErqpPTOX0B9WSKaBB5SMDMrbdSkEBGNkxWImZlNvaPpPjIzs2mmIpOCT51tZlZaRSYFMzMrrTKTghsKZmYlVWZSMDOzkioyKbihYGZWWkUmBTMzK81JwczMCioyKfgfzWZmpVVkUjAzs9IqMin4z2tmZqVVZFIwM7PSKjIpeEzBzKy0ikwKZmZWmpOCmZkVVGRScO+RmVlpFZkUzMystLIlBUmfk7RT0tqisjmSvifp6fR+dtFz10vaIOkpSZeUKy6A8EizmVlJ5WwpfAG4dETZdcB9EbEcuC99jKQVwJXA2ekyn5KUKWNsZmZWQtmSQkQ8AOwZUXw5cFs6fRtwRVH5nRHRHxGbgA3AqnLFZmZmpU32mML8iNgBkN7PS8sXAVuL5mtNyw4h6VpJqyWtbm9vn1AQ7j0yMyvteBloVomykrvuiLglIlZGxMqWlpYyh2VmVlkmOym0SVoAkN7vTMtbgSVF8y0Gtk9ybGZmFW+yk8I9wDXp9DXAN4rKr5RUK2kZsBx4eJJjMzOreNXlWrGkLwEXAXMltQJ/C9wM3CXprcBvgNcDRMQ6SXcBTwJDwDsjIleu2DymYGZWWtmSQkRcdZinXnyY+W8CbipXPGZmdmTHy0CzmZkdByoyKfgiO2ZmpVVkUjAzs9IqMil4oNnMrLSKTApmZlZaRSYFNxTMzEqryKRgZmalOSmYmVlBRSYFX2THzKy0ikwKZmZWWkUmBbcTzMxKq8ikYGZmpTkpmJlZQUUmBY8zm5mVVpFJwczMSqvQpOCmgplZKRWaFMzMrJSKTAoeUzAzK60ik4KZmZXmpGBmZgUVmRSKe496BoamLA4zs+NNRSaFYvc+vmOqQzAzO25UZFIoHmieUZOZukDMzI4zFZkUijXUVk91CGZmx42KTApRNKpQV+2WgpnZsIpMCsV8wR0zswOmpO9E0magE8gBQxGxUtIc4MvAUmAz8IaI2FvuWHJOCmZmBVPZUvi9iDgvIlamj68D7ouI5cB96eOyKM4DubyTgpnZsOOp++hy4LZ0+jbgisl4UTcUzMwOmKqkEMB3JT0i6dq0bH5E7ABI7+eVWlDStZJWS1rd3t4+sRd3S8HMrKSpOh7zBRGxXdI84HuSfjnWBSPiFuAWgJUrVx71Ht1jCmZmB0xJSyEitqf3O4GvAauANkkLANL7nZMRS94tBTOzgklPCpIaJDUOTwO/D6wF7gGuSWe7BvhGuWIo/p+CWwpmZgdMRffRfOBrkoZf/46I+LaknwN3SXor8Bvg9ZMRjMcUzMwOmPSkEBEbgXNLlO8GXjw5MZSeNjOrdMfTIalTwi0FM7MDnBTcVDAzK6j4pOCjj8zMDqj4pOCWgpnZARWZFIrzgFsKZmYHVGRSKOaBZjOzAyoyKRT/ec05wczsgIpMCsXyHlMwMyuoyKTgs6SamZVWkUmhmI8+MjM7oOKTgo8+MjM7oCKTwtK5DXz2LclVQHP5KQ7GzOw4UpFJobk+y4vPSi7s5u4jM7MDKjIpAEgimxEDQ24qmJkNq9ikADB7Rg23/2wLX/tFKxvbu6Y6HDOzKVfRSWFOQw2d/UO8/8uPcfFHf0S4K8nMKlxFJ4WRf1zr6h+aokjMzI4PFZ0U3n3xchprD1x87umd7kIys8pW0Unhlecu5PEbf58/ueg0AN70mYfY0z0wxVGZmU2dik4KkByF9MFLzuBjbziXvqEcL//Ej9mxv3eqwzIzmxLVR55l+pPEa85fTD7gA//5GBf+zx8A8OVrL+C3Tz1piqMzM5s8TgpFXve8xfQMDPE331gHwBtv+RmvPHchM2urObmpjpVLZ5PLJyfe/q1FzTTXZ9myu5u5jbX09OeYO7OGTJUYyOXZ2z1Iz8AQ9TUZ5jfWUVUldnb20TKzFkkARAQDuTxrt+3n5OZ6FjbXsb93kFkzaugbzFGXzbC7q585DTUM5YP+oTwza6uJiMI6tu3rpT6boaY6afTNLBojyecDicK8Iw3l8lRnDt9Y3NczQF02Q102c8i6Wvf2ML+pjuoqHXb9w4rjPRaGt02pdR/r1zKrNE4KI7zlwqW85vzF/PTXu/nEfU/zX49tP+p1LplTz9Y9B7qk6rJV9A0e/k9zzfVZ9vcOcurcBjbu6qYmU8XAiPNxNNRkGMzFIeULm+vY2dnPotn1bNndUyiXYGFzPVJyZti9PQOFGBbPrqe7f4ihXNCZHoF1+ryZbNjZxfD+dfhArWVzG6jPZnhyRwcA9dkMJzfXMTCUp62jj0Wz66nPZtjdPUB7Zz8vWj6XHz+9i7MWNNHSWEtddRV9Q3nOP2UW2/f10tU/xDefeIZTWxp40elzGcwH+3oG2NjezenzZhLp9hgYyjOYy7O3Z5AHftV+0LbKVIk93QOcMb+Rp9o6ueTs+TTWZbn/qZ001mWZNSObbPfqDKfPm0ldtoontu2nbzBPfTZDfU2GfT0D7OsZZOGsZBttbO+mpbGWLbu7eeHyFprrqxkYyjOjppr9vYPs6xngWSc18NQznezu7mf5/EY6+4bI54POvkFOa5nJmq372NszwLK5DdRUV7GwuZ7Gump2dvazoLmenoEhBoby5CM44+QmNrZ3sWh2PafMmcHWPb08uKGdOQ01bGzv5uTmOtZs3UdNpoqXrpjPoln1DOTy9A/leXJ7B20dfZy5oInm+mp2dw3wyJa9nNYyk+37e7lg2UlUVcHi2TPoH8rT3tnPrq5+7lvfxhknNyHg+UtnU1UlGuuyPN3WyblLZpGPYN32DmZkM2zf38uZJzexv3cQgPlNtfQNJrEvPamBzr5BdnUNMK+plk/f/2sIeO3zFrOzs49spopftXUxpyHL750xj8FcsHVvD19Z3corz11I32CO0+bNJJ8PZs3I0rq3l8Fc8n5XSVRViR37evnpxt386UWn01yf5fvr22ioqWZ2Q5ZdXQO0NNYyI5th1owsC2fVs7G9m4FcnlVL57B9f2/yGekaSOtYTVtHH8vmziSbEXu7B5jdUMNDm/awdtt+Vi2dw57uAda07mPFgiae96zZ7Orqp646w3NPmU3fYI6n2jpp3dvL/KZa8vlgb88gZ5zcyLzGWjbu6mZP9wDfWruDq1adwo59yTboG8rRXJ/l6bYuLj5zHlWCxrosG9u7+GVbJwub6zhnUTODuaBnYIjOviF2dvRx2ryZtHX0UZPJUJet4utrtnP1BafwiucspLk+O8a90NjpRD42f+XKlbF69eqyvkbfYI6fbtzN7q4BegeGWLN1P3Nn1rC3Z4BnOvpZ2FzHQC7PKXNmsHbbfnL5oG8wT/9QjucsnsVJDTU8vHkPu7sG6B4Y4pyFzcyakaWto4/vr98JwDmLmpiRraa9q59Nu7p56Yr5DAzl2by7my27e5jfVEtbR/9BcV185jyqq8R3n2yjoSZDS2Mtu7oGeMHpJ5HLQ3tnH4+17ud3n93Cj37VzoLmOprqsiyYVccTrfvZXTSgLh18OnGAmuoqzpjfSGNdsjNcvWUvAPMaa+kdyIHg5KY6qjNVhX+G//KZThbNqqc2W0XvQI4d+/s4a0ET69MEAgcS3vB0Lh+jHgrcUJOheyBHbXUV85vqGMzl2bG/j8a6ajr7xncI8ZGSMcCMmqRlNNoBB3Nn1rCr68gHJCyaVc+2fQd+DFRXiaFpcALGTJUQTIu6nMh+99kt3PbHqya0rKRHImJlqefcUjiCumyG3ztjXuHxmy+cwmCAjr5BZtZUU1U19i6SUl0q+XyQi6C6argr68BzfUM5spkqsiO6lsbbNbOne4A5DTWHrGMwF/QP5WioqUaC7fv7WDSrnsFcnuoq0TOQY0ZN0j0kic6+QbKZqkO6jPb3DtJcnyUi6Vobfr5vMMf2fb0sm9vAYC5pecxrqgOgu38ICaqrqpCSGOtrMvQP5qmprjrol9dgLs+2vb1kq6tY2FyXrjtPfU2GiGBfzyBd/UNEUPjFWpetonVvL+efMpsqQf9Qnke27GV39wCXnXMymSqRyyfboKNvkMa6anoGcoX3YSCXZ2dHP72DOXZ39TNrRg2LZ9cX4n1yewdnLmhEJK2jfARL5zZQJaitzrB9Xy8LZ9WzbW8vddkq2jr6WTKnntrqDJ39gzTVZdnTPUA2U8W2fb08a84MpGRHX11VRT6C9s5+ZtRmWLetg+qMWDSrnnwk85zcVEc2IwZzwfb9vWze1c1gLjh93kwAHt2yl4vPmkc+go3t3eTzwe+cPpe93QNs2dPDlt3dnH/KbObOrKWmuopNu7qZ01BDd/8QW3b3kM2IXARzZ9ZSn82ws7OfM09uZOveHmbV1/BY6z7y+WD5/EZ6Bobo7s/xW4ubae/sR0B1Rqzdtp85DbX8bONulsypp0qiLpth/Y4OXrpiPrXVVUmrLqChNsO2vb187RfbeOHpc1k+fyazZ9TQN5ineUaWbXt7aajN0FBTzZ6eATa1d9NYV815S2axt2eQ2uoqhvLBj37VzsVnzmNXVz9Pt3Vx9sImAH741E627+vlQ5efw0Aun7S+gSqJTJX48s+3snLpbJ6zeBb7ewfZ2zPA/p5B2jr6OHfJLL67ro3ZM7K84flL+NxPNiHEK56zgH09g8xtPPi7dawcdy0FSZcC/wJkgM9GxM2Hm3cyWgpmZtPNaC2F4+qQVEkZ4N+AlwErgKskrZjaqMzMKsdxlRSAVcCGiNgYEQPAncDlUxyTmVnFON6SwiJga9Hj1rSsQNK1klZLWt3e3o6ZmR07x1tSKDWKedCgR0TcEhErI2JlS0vLJIVlZlYZjrek0AosKXq8GDj6PwqYmdmYHG9J4efAcknLJNUAVwL3THFMZmYV47j6n0JEDEl6F/AdkkNSPxcR66Y4LOIE950AAAYfSURBVDOzinFcJQWAiPgm8M2pjsPMrBIdd39eGw9J7cCWo1jFXGDXMQrnRFBp9QXXuVK4zuPzrIgoeaTOCZ0Ujpak1Yf7V990VGn1Bde5UrjOx87xNtBsZmZTyEnBzMwKKj0p3DLVAUyySqsvuM6VwnU+Rip6TMHMzA5W6S0FMzMr4qRgZmYFFZkUJF0q6SlJGyRdN9XxHCuSlkj6oaT1ktZJem9aPkfS9yQ9nd7PLlrm+nQ7PCXpkqmLfuIkZST9QtK96ePpXt9Zkr4i6Zfpe31hBdT5/elneq2kL0mqm251lvQ5STslrS0qG3cdJT1P0hPpc5/QeC6XCMmlDSvpRnL6jF8DpwI1wGPAiqmO6xjVbQFwfjrdCPyK5GJF/wu4Li2/DvjHdHpFWv9aYFm6XTJTXY8J1PvPgDuAe9PH072+twFvS6drgFnTuc4kp8/fBNSnj+8C/nC61Rn4b8D5wNqisnHXEXgYuJDkrNPfAl42njgqsaUwbS/kExE7IuLRdLoTWE/yhbqcZEdCen9FOn05cGdE9EfEJmADyfY5YUhaDLwc+GxR8XSubxPJzuNWgIgYiIh9TOM6p6qBeknVwAySsydPqzpHxAPAnhHF46qjpAVAU0T8NJIM8b+LlhmTSkwKR7yQz3QgaSnwXOAhYH5E7IAkcQDz0tmmw7b4Z+CDQL6obDrX91SgHfh82mX2WUkNTOM6R8Q24CPAb4AdwP6I+C7TuM5FxlvHRen0yPIxq8SkcMQL+ZzoJM0E7gbeFxEdo81aouyE2RaSXgHsjIhHxrpIibITpr6papIuhk9HxHOBbpJuhcM54euc9qNfTtJNshBokHT1aIuUKDuh6jwGh6vjUde9EpPCtL6Qj6QsSUK4PSK+mha3pc1K0vudafmJvi1eALxK0maSbsCLJX2R6VtfSOrQGhEPpY+/QpIkpnOdXwJsioj2iBgEvgr8DtO7zsPGW8fWdHpk+ZhVYlKYthfySY8yuBVYHxEfK3rqHuCadPoa4BtF5VdKqpW0DFhOMkh1QoiI6yNicUQsJXkffxARVzNN6wsQEc8AWyWdkRa9GHiSaVxnkm6jCyTNSD/jLyYZL5vOdR42rjqmXUydki5It9VbipYZm6kecZ+iUf7LSI7M+TXwV1MdzzGs1wtJmoqPA2vS22XAScB9wNPp/ZyiZf4q3Q5PMc6jFI6nG3ARB44+mtb1Bc4DVqfv89eB2RVQ578DfgmsBf4PyVE306rOwJdIxkwGSX7xv3UidQRWptvp18AnSc9cMdabT3NhZmYFldh9ZGZmh+GkYGZmBU4KZmZW4KRgZmYFTgpmZlbgpGAVTVJXer9U0h8c43XfMOLx/zuW6zcrBycFs8RSYFxJQVLmCLMclBQi4nfGGZPZpHNSMEvcDLxI0pr03P0ZSf8k6eeSHpf0dgBJFym5ZsUdwBNp2dclPZKe7//atOxmkrN6rpF0e1o23CpRuu616Xnv31i07vuLrpVw+/C58CXdLOnJNJaPTPrWsYpRPdUBmB0nrgM+EBGvAEh37vsj4vmSaoGfSPpuOu8q4JxITlkM8McRsUdSPfBzSXdHxHWS3hUR55V4rdeQ/Cv5XGBuuswD6XPPBc4mOV/NT4AXSHoSeDVwZkSEpFnHvPZmKbcUzEr7feAtktaQnH78JJLzy0ByjplNRfO+R9JjwM9ITlK2nNG9EPhSROQiog34EfD8onW3RkSe5DQlS4EOoA/4rKTXAD1HXTuzw3BSMCtNwLsj4rz0tiySc/hDcrrqZCbpIpKzeF4YEecCvwDqxrDuw+kvms4B1RExRNI6uZvkginfHldNzMbBScEs0UlyCdNh3wH+JD0VOZKenV7MZqRmYG9E9Eg6E7ig6LnB4eVHeAB4Yzpu0UJyJbXDnsUzvT5Gc0R8E3gfSdeTWVl4TMEs8TgwlHYDfQH4F5Kum0fTwd52Sl/W8NvAOyQ9TnK2yp8VPXcL8LikRyPiTUXlXyO5hu5jJGe1/WBEPJMmlVIagW9IqiNpZbx/YlU0OzKfJdXMzArcfWRmZgVOCmZmVuCkYGZmBU4KZmZW4KRgZmYFTgpmZlbgpGBmZgX/H7Xmn2Y2WmEaAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open('trained_weights/vgg16_ssd_stats_adam.pkl','rb') as f:\n",
    "    l_loss, c_loss, itr = pickle.load(f)\n",
    "\n",
    "l_loss = np.asarray(l_loss)\n",
    "c_loss = np.asarray(c_loss)\n",
    "itr = np.asarray(itr)\n",
    "plt.plot(itr,l_loss+c_loss)\n",
    "plt.title('Loss - Iterations: lr=1e-3  adam Optimizer')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
